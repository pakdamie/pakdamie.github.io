[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Damie Pak, PhD",
    "section": "",
    "text": "Hello, I am Damie (Dah-mie) Pak. I am a quantitative/theoretical ecologist with a special interest in population dynamics—both pest and infectious disease. I consider myself a generalist, interested in applying ecology theory and applying it to diverse systems. I believe the most interesting questions in ecology lie at the intersection of theory and application. My main tools are mathematical models and data analysis; with each project, I strive to learn a new analytical tool (wavelet analysis, phylogenetic regression, network analysis, etc.) Check out the research tab to see more of my work!\nI received my doctorate from The Pennsylvania State University, where I worked with Dr. Ottar Bjornstad to investigate how climate drives phenology of insect pests. As a postdoc, I became interested in infectious diseases. After graduating, I worked with Dr. Kat Shea to model vaccine prioritization under uncertainty. I then moved to Cornell to collaborate with Dr. Megan Greischar on within-host modeling of malaria. Currently, I am a postdoctoral fellow at the University of South Carolina, working with Dr. Tad Dallas on various disease-related projects. Check out the research tab to see more of my work!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "CV",
    "section": "",
    "text": "&lt;object data= “pak_CV.pdf”, width=“1000” height=“1000” type=‘application/pdf’/&gt;"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog posts",
    "section": "",
    "text": "Simulating ecology models: A series of tutorial (Part 2)\n\n\nGrowth can’t be unlimited!\n\n\n\nCode\n\n\nTutorial\n\n\n\nAn introduction to the logistic equation\n\n\n\n\n\nOct 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCompound probability and demographic stochasticity\n\n\nWhat is it\n\n\n\nTutorial\n\n\nCode\n\n\n\nThere’s a compound probability appearing in a Ricker Poisson model! Oh no!\n\n\n\n\n\nOct 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAn agent based model of within-host malaria\n\n\nAn example\n\n\n\nMaterial\n\n\n\nA short introduction to doing an agent-based malaria model\n\n\n\n\n\nOct 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nExpected value of perfect information\n\n\nWhat is it\n\n\n\nMaterial\n\n\n\nA simple vizualization\n\n\n\n\n\nOct 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSecondary vectors: when David becomes Goliath\n\n\n\n\n\n\nEssay\n\n\n\nSecondary vectors should be a concern for climate change\n\n\n\n\n\nOct 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSimulating ecology models: A series of tutorial\n\n\nPopulation growth: discrete and continuous\n\n\n\nCode\n\n\nTutorial\n\n\n\nHow to model a single-species population the first time\n\n\n\n\n\nOct 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic SIS model\n\n\nLearning for me\n\n\n\nTutorial\n\n\n\nA short tutorial on how to model stochastic SIS model on a network.\n\n\n\n\n\nSep 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSimulating a scale-free network\n\n\nVarying connectance…\n\n\n\nCode\n\n\nTutorial\n\n\n\nOne possible (possibly bad) way of modeling a spatial network that is also scale-free\n\n\n\n\n\nAug 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nA proof of concept: a coin-toss game\n\n\nThe most toy model\n\n\n\nCode\n\n\n\nWhat I thought was a clever analogy of insects at hotter temperatures\n\n\n\n\n\nAug 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSimulating spatial network (Part 2)\n\n\n\n\n\n\nTutorial\n\n\nCode\n\n\n\nHow to simulate a semi-realistic spatial network, maybe in kind of a hackey way\n\n\n\n\n\nJul 31, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIt’s weird we don’t talk about Malthus\n\n\nNo but seriously.\n\n\n\nEssay\n\n\n\nMalthus is associated with exponential growth… and bad famine policies that led to the death of millions. Why don’t we ever talk about it in Intro to Ecology classes?\n\n\n\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nFinite versus instantaneous rate\n\n\nWhy you gotta convert\n\n\n\nTutorial\n\n\n\nFinite and instanteous rates can be confusing especially when we talk about mortality rates.\n\n\n\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGRFP Statements\n\n\nAn example\n\n\n\nMaterial\n\n\n\nGRFP material for a theoretical ecology project\n\n\n\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson process\n\n\nLearning for me\n\n\n\nTutorial\n\n\n\nA short tutorial on understanding the poisson process\n\n\n\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGuest lecture slides for teaching the Lotka-Volterra Predator/Prey dynamics\n\n\n\n\n\n\nSlides\n\n\nMaterial\n\n\n\nGuest lecture slides for teaching\n\n\n\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nExponential waiting time\n\n\n\n\n\n\nTutorial\n\n\nModeling\n\n\nBeginner\n\n\n\nWhy do we assume that the waiting time in a compartmental model is exponentially distributed?\n\n\n\n\n\nJul 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAbouheif Mean C\n\n\nQuick pic to understand\n\n\n\nMaterial\n\n\n\nAbouheif Mean C is one way to check if there is a phylogenetic signal: how does it work?\n\n\n\n\n\nJul 1, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "In the particular is contained the universal. - James Joyce\nBelow are some projects I have pursued:"
  },
  {
    "objectID": "research.html#side-by-side-layout",
    "href": "research.html#side-by-side-layout",
    "title": "Research",
    "section": "",
    "text": "Here is some text that you want to place next to a figure. You can write any content here."
  },
  {
    "objectID": "research.html#temperature-and-insect-outbreak",
    "href": "research.html#temperature-and-insect-outbreak",
    "title": "Research",
    "section": "Temperature and insect outbreak",
    "text": "Temperature and insect outbreak\n\n\n\n\n\n\nFor my doctoral work with Dr. Ottar Bjornstad at Penn State, I worked on understanding the dynamics of insect outbreaks. Specifically, I am interested in the role of climate on driving the phenology of tortricid moth pests. The spring emergence of the adult stages, for example, is influenced both by local temperatures and large scale-climate oscillations such as the North Atlantic Oscillation and the Arctic Oscillation and Arctic Oscillation (Local and Regional Climate Variables Driving Spring Phenology of Tortricid Pests: A 36-Year Study). Each tortricid species, because of their unique life-histories, are sensitive to these cues at different temporal windows. For example, species with multiple generations"
  },
  {
    "objectID": "research.html#tick-community",
    "href": "research.html#tick-community",
    "title": "Research",
    "section": "Tick community",
    "text": "Tick community\n\n\n\n\n\nFor my doctoral work with Dr. Ottar Bjornstad, I"
  },
  {
    "objectID": "research.html#the-vector-community-community",
    "href": "research.html#the-vector-community-community",
    "title": "Research",
    "section": "The vector community community",
    "text": "The vector community community\n\n\n\n\n\nFor my doctoral work with Dr. Ottar Bjornstad, I\n\n\n\n\n\n\n\n\nFor my doctoral work with Dr. Ottar Bjornstad, I"
  },
  {
    "objectID": "research.html#a-community-perspective-of-vectors",
    "href": "research.html#a-community-perspective-of-vectors",
    "title": "Research",
    "section": "A community perspective of vectors",
    "text": "A community perspective of vectors\n\n\n\n\n\n\nI’m fascinated by communities of vector species! When investigating vector-borne diseases, we tend to focus on one species at a time, but there can be multiple vectors, each with its own unique ecology. In collaboration with Dr. Joyce Sakamoto, we analyzed a century’s worth of tick data (A 117-year retrospective analysis of Pennsylvania tick community dynamics). We showcased how the dominant vector species have changed over the century, as well as the spatial partitioning between vector species.\n\n\n\n\n\n\n\n\nCurrently with my postdoctoral work with Tad Dallas, a project I’m working on is how disturbance can structure the relative abundance of different vectors."
  },
  {
    "objectID": "research.html#trees-trees-trees",
    "href": "research.html#trees-trees-trees",
    "title": "Research",
    "section": "Trees, trees, trees",
    "text": "Trees, trees, trees\n\n\n\n\n\nFor my doctoral work with Dr. Ottar Bjornstad, I"
  },
  {
    "objectID": "research.html#ecological-management",
    "href": "research.html#ecological-management",
    "title": "Research",
    "section": "Ecological management",
    "text": "Ecological management\n\n\nFor my short six-month postdoc with Dr. Katriona Shea (because moving doing COVID times is difficult), I learned about decision theory and how to apply it for vaccine allocation. Specifically, under significant uncertainty- what is the best course of action? Is there always going to be an optimal choice?"
  },
  {
    "objectID": "research.html#within-host-modelig",
    "href": "research.html#within-host-modelig",
    "title": "Research",
    "section": "Within-host modelig",
    "text": "Within-host modelig\n\n\n\n\n\nFor my doctoral work with Dr. Ottar Bjornstad, I"
  },
  {
    "objectID": "research.html#ecological-management-1",
    "href": "research.html#ecological-management-1",
    "title": "Research",
    "section": "Ecological management",
    "text": "Ecological management"
  },
  {
    "objectID": "posts/helloworld.html",
    "href": "posts/helloworld.html",
    "title": "Damie Pak",
    "section": "",
    "text": "Hello this is a blog post"
  },
  {
    "objectID": "research.html#reproductive-phenology",
    "href": "research.html#reproductive-phenology",
    "title": "Research",
    "section": "Reproductive phenology",
    "text": "Reproductive phenology\n\n\n\n\n\n\nCollaborating with Dr. Jesse Lasky, I was interested in looking at phenology in another system (Multiscale phenological niches of seed fall in diverse Amazonian plant communities)."
  },
  {
    "objectID": "research.html#tortricid-moth-pests",
    "href": "research.html#tortricid-moth-pests",
    "title": "Research",
    "section": "Tortricid moth pests",
    "text": "Tortricid moth pests\n\n\n\n\n\n\nFor my doctoral work with Dr. Ottar Bjornstad at Penn State, I worked on understanding the dynamics of insect outbreaks. Specifically, I am interested in the role of climate on driving the phenology of tortricid moth pests. The spring emergence of the adult stages, for example, is influenced both by local temperatures and large scale-climate oscillations such as the North Atlantic Oscillation and the Arctic Oscillation (Local and Regional Climate Variables Driving Spring Phenology of Tortricid Pests: A 36-Year Study). Each tortricid species, because of their unique life-histories, are sensitive to these cues at different temporal windows. Additionally, I also use mathematical modeling to understand how diapause, a state of suspended development, can be incorporated for long-term forecasting (Incorporating Diapause to Predict the Interannual Dynamics of an Important Agricultural Pest)"
  },
  {
    "objectID": "research.html#reproductive-phenology-of-neotropical-trees",
    "href": "research.html#reproductive-phenology-of-neotropical-trees",
    "title": "Research",
    "section": "Reproductive phenology of neotropical trees",
    "text": "Reproductive phenology of neotropical trees\n\n\nIn my collaboration with Dr. Jesse Lasky (Multiscale phenological niches of seed fall in diverse Amazonian plant communities), we applied wavelet analysis to long-term time series data from two hyperdiverse plant communities in Peru and Ecuador. We analyzed the temporal patterns of synchrony and compensatory dynamics in seed dispersal. Our findings show that there is community-wide reproductive synchrony due to shared environmental responses and positive interactions. Additionally, there is evidence of temporal niche partitioning!"
  },
  {
    "objectID": "research.html#within-host-modeling",
    "href": "research.html#within-host-modeling",
    "title": "Research",
    "section": "Within-host modeling",
    "text": "Within-host modeling\n\n\n\n\n\nFor my doctoral work with Dr. Ottar Bjornstad, I"
  },
  {
    "objectID": "research.html#climate-and-tortricid-moth-pests",
    "href": "research.html#climate-and-tortricid-moth-pests",
    "title": "Research",
    "section": "Climate and tortricid moth pests",
    "text": "Climate and tortricid moth pests\n\n\n\n\n\n\nFor my doctoral work with Dr. Ottar Bjornstad, I investigated how the environment (local temperature) influences the phenology of insect pests. For example, by using a mathematical model, I demonstrated how temperature can predict the phenology of the codling moth (Incorporating Diapause to Predict the Interannual Dynamics of an Important Agricultural Pest). Additionally, by analyzing the time series of five pest species in Pennsylvania, I showed how their phenology is driven by two large scale-climate oscillations: the North Atlantic Oscillation and the Arctic Oscillation (Local and Regional Climate Variables Driving Spring Phenology of Tortricid Pests: A 36-Year Study)."
  },
  {
    "objectID": "research.html#within-host-modeling-of-malaria",
    "href": "research.html#within-host-modeling-of-malaria",
    "title": "Research",
    "section": "Within-host modeling of malaria",
    "text": "Within-host modeling of malaria\n\n\n\n\n\n\nIn my postdoctoral work with Dr. Megan Greischar, I investigated an understudied trait: the burst size of malaria parasites. The burst size determines the number of daughter cells that emerge from each infected RBC. Because a larger burst size allows the parasite to proliferate faster, an interesting question arises: why is it not higher? (Proliferation in malaria parasites: how resource limitation can prevent the evolution of greater virulence). I am also finishing a manuscript that investigates the role of burst size across all parasite species."
  },
  {
    "objectID": "posts/coin_flipping/coin_flip.html",
    "href": "posts/coin_flipping/coin_flip.html",
    "title": "A proof of concept: a coin-toss game",
    "section": "",
    "text": "This is a proof-of-concept for a research project I’m developing. Therefore, this is written more for me than for a general audience (sorry!). I can’t give it all away now, but hopefully it’s a taste of what I’m doing. Instead of an ecological question, let’s imagine you are hosting a BBQ and you somehow got a large group of your friends to play a lawn game. Everyone is given a coin and told to flip. If you get heads, you proceed one step. If you get tails, you die (specifically, you just lay down and stay in place). You ‘win’ if you can make it 10 paces away. Here is a schematic below:\n\nBut let’s make it more interesting! Let’s assume that the coin can be super biased. Instead of a 50% chance of dying, I manipulate it so that the chance of getting tails can vary from 1% to 100%. Also let’s assume that I introduce some variability. Some friends can only take very small steps and other friends can take larger steps. They somehow need to cumulatively take 10 paces to win, but you can see that there are advantages to those who can take very large steps.\nMy question is what does it look like at the end of the finish line. Specifically, what are the group of individuals that are able to finish (i.e do they take small steps or big steps?) and what is the timing? How does this differ depending on what kind of mortality coin I give them and what kind of steps I allow them to take?"
  },
  {
    "objectID": "posts/coin_flipping/coin_flip.html#a-more-realistic-math-ier-one",
    "href": "posts/coin_flipping/coin_flip.html#a-more-realistic-math-ier-one",
    "title": "A proof of concept: coin-toss game",
    "section": "A more realistic math-ier one",
    "text": "A more realistic math-ier one"
  },
  {
    "objectID": "posts/coin_flipping/coin_flip.html#putting-it-all-together",
    "href": "posts/coin_flipping/coin_flip.html#putting-it-all-together",
    "title": "A proof of concept: coin-toss game",
    "section": "Putting it all together",
    "text": "Putting it all together\n\ndeath_function &lt;- function(size, mort_prob){\n  sampled &lt;- sample(c(0,1), size,replace = TRUE, prob =c(mort_prob, 1- mort_prop))\n  return(sampled)\n}\n\n\ndevelopment_function &lt;- function(size, prob, id){\n  \n  if(id ==1){\n  sampled &lt;- runif(seq(0,1), size, replace = TRUE,)\n  }\n  else if (id ==2){\n  sampled &lt;- runif(seq(1,2), size, replace = TRUE,)\n  }\n  else if (id ==3){\n  sampled &lt;- runif(seq(2,3), size, replace = TRUE,)\n  }\n  return(sampled)\n}"
  },
  {
    "objectID": "posts/coin_flipping/coin_flip.html#an-introduction",
    "href": "posts/coin_flipping/coin_flip.html#an-introduction",
    "title": "A proof of concept: a coin-toss game",
    "section": "",
    "text": "This is a proof-of-concept for a research project I’m developing. Therefore, this is written more for me than for a general audience (sorry!). I can’t give it all away now, but hopefully it’s a taste of what I’m doing. Instead of an ecological question, let’s imagine you are hosting a BBQ and you somehow got a large group of your friends to play a lawn game. Everyone is given a coin and told to flip. If you get heads, you proceed one step. If you get tails, you die (specifically, you just lay down and stay in place). You ‘win’ if you can make it 10 paces away. Here is a schematic below:\n\nBut let’s make it more interesting! Let’s assume that the coin can be super biased. Instead of a 50% chance of dying, I manipulate it so that the chance of getting tails can vary from 1% to 100%. Also let’s assume that I introduce some variability. Some friends can only take very small steps and other friends can take larger steps. They somehow need to cumulatively take 10 paces to win, but you can see that there are advantages to those who can take very large steps.\nMy question is what does it look like at the end of the finish line. Specifically, what are the group of individuals that are able to finish (i.e do they take small steps or big steps?) and what is the timing? How does this differ depending on what kind of mortality coin I give them and what kind of steps I allow them to take?"
  },
  {
    "objectID": "posts/coin_flipping/coin_flip.html#the-simplest-code",
    "href": "posts/coin_flipping/coin_flip.html#the-simplest-code",
    "title": "A proof of concept: a coin-toss game",
    "section": "The simplest code",
    "text": "The simplest code\nFor the death function, I’m going to use sample until I think a bit more about the gritty mathematics. There is a binary outcome: you survived in this timestep or you perished in this timestep. But we can directly manipulate the probability of mortality.\n\ndeath_function &lt;- function(size, mort_prob){\n  sampled &lt;- sample(c(0,1), size,replace = TRUE, prob =c(mort_prob, 1- mort_prob))\n  return(sampled)\n}\n\nNow, how do our friends progress through the lawn game. We can give everyone a number (1,2,3) and depending on your number, you can take large steps or very small. You can see that if you are in Group 1, you take smaller steps than individuals in Group 3.\n\nprogress_function &lt;- function(id){\n  \n  if(id ==1){\n  sampled &lt;- runif(1,min =0, max =3)\n  }\n  else if (id ==2){\n  sampled &lt;- runif(1,min = 2,max=5)\n  }\n  else if (id ==3){\n  sampled &lt;- runif(1,min = 5, max =10)\n  }\n  return(sampled)\n}\n\nNow here is the most convoluted code of how the race can begin. A gist of it is that for anyone who has not died, I make them flip the coin. If it’s 0, they stay in place and I record at what time they ‘died’. If it’s a 1, they are still alive where they can make progress to winning. If they accumulate 10 steps, they won and wait while everyone finishes (by either ‘dying’ or ‘winning’).\n\n\nCode\nlawn_race_function &lt;- function(size, mort_prob, progress_time,time_step) {\n  full_df &lt;- data.frame(\n      current_time = rep(0, size),\n      time_event = rep(0, size),\n      status = rep(1, size),\n      progress = rep(0, size),\n       id = rep(c(1, 2, 3), length = size),\n       friend_number = seq(1, size))\n\n  survived_subsetted &lt;- full_df [full_df $status == 1 ,]\n  \n  i = 0\n  \n  while (nrow(full_df [full_df $status == 1 ,]) != 0) {\n    \n    i = i + 1\n    dead_developed_already_subsetted &lt;- full_df[full_df$status %in% c(0,2),]\n    survived_subsetted &lt;- full_df [full_df $status == 1 ,]\n    survived_individuals &lt;- nrow(survived_subsetted)\n ###If there are surviving individuals\n      \n      survived_subsetted$current_time &lt;- i\n\n      ### Did they die in this time-step?\n      survived_subsetted$status &lt;- death_function(survived_individuals, mort_prob)\n      \n      if(nrow(  survived_subsetted[survived_subsetted$status == 0, ])!=0){\n      \n      survived_subsetted[survived_subsetted$status == 0, ]$time_event &lt;- i\n\n      }\n      ### Progressed\n      growing_indviduals &lt;- survived_subsetted[survived_subsetted$status == 1, ]\n\n      ###If there are those that will grow\n      if(nrow(growing_indviduals) !=0){\n      \n      growing_indviduals$progress &lt;- round(growing_indviduals $progress + \n                                            time_step*sapply(X = growing_indviduals $id, FUN = progress_function ), 3)\n\n      developed &lt;- growing_indviduals[growing_indviduals$progress &gt;= progress_time, ]\n\n      ###If there are those developed \n      if(nrow(developed) != 0){\n      growing_indviduals[growing_indviduals$progress &gt;= progress_time, ]$status &lt;- 2\n      growing_indviduals[growing_indviduals$status == 2, ]$time_event &lt;- i\n      }\n      }\n      \n      full_df &lt;- rbind(dead_developed_already_subsetted ,\n        survived_subsetted[survived_subsetted$status == 0, ],\n       growing_indviduals\n      )\n  \n  \n  }\n  return(full_df)\n}"
  },
  {
    "objectID": "posts/coin_flipping/coin_flip.html#the-result",
    "href": "posts/coin_flipping/coin_flip.html#the-result",
    "title": "A proof of concept: a coin-toss game",
    "section": "The result",
    "text": "The result\nSo I set two matches with 100,000 of my friends. The first match, the probability of dying at each time step is 0.001 and you must make 10 steps to win. The second match, the probability of dying at each time step is 0.10 and still you must take 10 steps to win. What does it look like between the two races?\n\ndf_lawn_race1&lt;- lawn_race_function(1e5,0.01,10,time_step = 1/5)\ndf_lawn_race2&lt;- lawn_race_function(1e5,0.40,10,time_step = 1/5)\n\n\nfirst_panel / second_panel +   plot_layout(guides = 'collect')\n\n\n\n\n\n\n\n\nSo it may be intuitive, but I like having this simulation and figure. When you’re in a race where the chance of mortality is very small, all groups are able to effectively ‘win’. The slower group (Group 3 with the star symbol!) may take a lot longer but eventually they will reach the finish line. However, in the second situation (the bottom graph), with greater chance for mortality, the one who are able to finish the race faster (Group 1) are more likely to win. With each time step.\nLet’s take a new perspective\nSo everyone who won in the game, what does it look as a cumulative proportion over time. The first race (dark green) and second race (blue) are quite different!\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\nggplot(df_full, aes(x = time_event, y= prop, color = as.factor(facet), group = as.factor(facet)))+\n geom_line(size =1.2)+\n  xlab(\"Time\")+\n  ylab(\"Cumulative proportion winning\")+\n  scale_color_manual(name = \"Race\",values = c(`1` = 'darkgreen', `2` = 'blue'))+\n  theme_classic()+\n theme(axis.text = element_text(size = 14),\n        axis.title = element_text(size = 14))+xlim(0,60)\n\n\n\n\n\n\n\n\nHuh… Could there be more variability of individuals winning in the first race?"
  },
  {
    "objectID": "posts/interesting_paper_week/interestingpaper1.html",
    "href": "posts/interesting_paper_week/interestingpaper1.html",
    "title": "Interesting stuff I read (August 12)",
    "section": "",
    "text": "Here is stuff that I read around this week and last week. Most of the readings are related to science and my job (ew), but some are just related to more of my hobbies and interests."
  },
  {
    "objectID": "posts/interesting_paper_week/interestingpaper1.html#simplicity-and-complexity-in-ecological-data-analysis",
    "href": "posts/interesting_paper_week/interestingpaper1.html#simplicity-and-complexity-in-ecological-data-analysis",
    "title": "Interesting stuff I read (August 12)",
    "section": "Simplicity and complexity in ecological data analysis",
    "text": "Simplicity and complexity in ecological data analysis\nPaul A. Martaugh 2007\nI really liked this paper because it takes a lot of balls to go back to your old papers and criticize your own statistical analyses. The author explains what he did for his original statistics then shows how simpler tests are better for explanation. Though, I’m still unsure about the second case study where he originally used a mixed-effect model to investigate the slope parameter between some body measurement and temperature for 53 fishes. His alternative is to run regression for each of the fishes and then average the slope parameter. I actually find the first test to be easier to understand. Hm."
  },
  {
    "objectID": "posts/interesting_paper_week/interestingpaper1.html#how-development-and-survival-combine-to-determine-the-thermal-sensitivity-of-insects",
    "href": "posts/interesting_paper_week/interestingpaper1.html#how-development-and-survival-combine-to-determine-the-thermal-sensitivity-of-insects",
    "title": "Interesting stuff I read (August 12)",
    "section": "How development and survival combine to determine the thermal sensitivity of insects",
    "text": "How development and survival combine to determine the thermal sensitivity of insects\nAbarca et al. 2024\nI really loved this paper because it’s something I’ve been thinking about for a project I’m trying to start. Basically, the premise is that looking at thermal performance curves for mortality and development by themselves say nothing about their performance. For example, in colder areas, development rate can be more of the limiting factor than survivorship. However, in warmer areas, survivorship can be more of the limiting factor. This can be an interesting look at Schmaulhausen law…"
  },
  {
    "objectID": "posts/interesting_paper_week/interestingpaper1.html#diptera-vectors-of-avian-haemosporidian-parasites-untangling-parasite-life-cycles-and-their-taxonomy",
    "href": "posts/interesting_paper_week/interestingpaper1.html#diptera-vectors-of-avian-haemosporidian-parasites-untangling-parasite-life-cycles-and-their-taxonomy",
    "title": "Interesting stuff I read (August 12)",
    "section": "Diptera vectors of avian Haemosporidian parasites: untangling parasite life cycles and their taxonomy",
    "text": "Diptera vectors of avian Haemosporidian parasites: untangling parasite life cycles and their taxonomy\nAlarcon et al. 2012\nReally interesting paper (though I skipped a lot of just to read the Plasmodium part). Shows that a lot of malaria knowledge can be beliefs instead of facts. A plea for more network analysis in parasite/vectors."
  },
  {
    "objectID": "posts/interesting_paper_week/interestingpaper1.html#thermal-biology-of-mosquito-borne-disease",
    "href": "posts/interesting_paper_week/interestingpaper1.html#thermal-biology-of-mosquito-borne-disease",
    "title": "Interesting stuff I read (August 12)",
    "section": "Thermal biology of mosquito-borne disease",
    "text": "Thermal biology of mosquito-borne disease\nMordecai et al. 2019\nTrying to work on a paper about thermal performance curves and trying to figure out what’s been done! Great paper, but makes me want to pull my hair out- what hasn’t been done?"
  },
  {
    "objectID": "posts/interesting_paper_week/interestingpaper1.html#evolution-of-geographic-variation-in-thermal-performance-curves-in-the-face-of-climate-change-and-implications-for-biotic-interactions",
    "href": "posts/interesting_paper_week/interestingpaper1.html#evolution-of-geographic-variation-in-thermal-performance-curves-in-the-face-of-climate-change-and-implications-for-biotic-interactions",
    "title": "Interesting stuff I read (August 12)",
    "section": "Evolution of geographic variation in thermal performance curves in the face of climate change and implications for biotic interactions",
    "text": "Evolution of geographic variation in thermal performance curves in the face of climate change and implications for biotic interactions\nTuzun and Stoks 2018\nHm, wild finding is that thermal performance curves do not vary across geography for a species with wide geographic distribution. The difference in TPC’s seem to be mostly ‘vertical’."
  },
  {
    "objectID": "posts/diff_eq1/diff_eq_1.html",
    "href": "posts/diff_eq1/diff_eq_1.html",
    "title": "Simulating ecology models: A series of tutorial",
    "section": "",
    "text": "This is a simple blog post series for those new to R and ecological models. This series go through the mathematics and the R codes necessary for simulation. This is catered towards people with experience with ecology but would like a casual introduction to mathematical modeling."
  },
  {
    "objectID": "posts/diff_eq1/diff_eq_1.html#introduction",
    "href": "posts/diff_eq1/diff_eq_1.html#introduction",
    "title": "Simulating ecology models: A series of tutorial",
    "section": "",
    "text": "This is a simple blog post series for those new to R and ecological models. This series go through the mathematics and the R codes necessary for simulation. This is catered towards people with experience with ecology but would like a casual introduction to mathematical modeling."
  },
  {
    "objectID": "posts/diff_eq1/diff_eq_1.html#lets-start-at-the-very-beginning",
    "href": "posts/diff_eq1/diff_eq_1.html#lets-start-at-the-very-beginning",
    "title": "Simulating ecology models: A series of tutorial",
    "section": "Let’s start at the very beginning",
    "text": "Let’s start at the very beginning\nA very good place to start is with population growth. Ecologists are fascinated by the rise and fall in the organism’s numbers. The simplest question an ecologist may start with: “If I have these initial number of organisms, and each produce this number of offspring, what should the total population be some time later?”\nWe use differential equations to model the change in the population size. Think of a second, now think of a nanosecond, and now think of the smallest slice of that nanosecond- the population is changing even within that tiny little time-frame. That may seem far-fetched. Generally, people are better about thinking of population growth in discrete time where with each discrete time-step (a day, a week, a month), there is an increase in the population.\nLet us say that our organisms of interest are female rabbits and we describe their population size at time \\(t\\) as \\(N_t\\). Assume that every with each time step, each rabbit births the same number of new rabbits! This parameter is the birth rate \\(b\\). If we have the initial founding number as \\(N_0\\) then we can write that the next time step as:\n\\[\nN_{1} = N_0 + bN_0.\n\\]\nFor example, if there was originally 5 adults (\\(N_0 = 5\\)) and each produces 5 rabbits (\\(b = 5\\)), then we will have 25 new adults. Adding both parents and children, we have \\(N_1 = 5 + (25) = 30\\) Here, we assume that the offsprings are adults pretty much instantly. What does this look like for \\(t = 2\\)? It would look like:\n\\[\nN_{2} = N(1) + 5N(1).\n\\]If we substitute \\(N(1) = 30\\), then we have \\(N_2 = 30 + 150 = 180\\). It is extremely tedious of having to write each time-step separately (imagine having to do this for a 100 timesteps!)\nBy rearranging the equation, we can see (try it out first!)\n\\[\nN_{1} = N_0(1 + b)\n\\]\n\\[\nN_{2} = N_{1}(1 + b).\n\\]\nThis suggests that:\n\\[\nN_{2} = N_0(1+b)(1+b).\n\\]\nSo how about \\(N_{3}\\)? It would then be:\n\\[\nN_{3} = N_{2} (1+b),\n\\]\nwhich is equivalent to:\n\\[\nN_{3} = N_0(1+b)(1+b)(1+b).\n\\]\nThat’s interesting! We basically just need to know the initial starting value (\\(N_0\\)) and we can raise the \\((1+b)\\) to the power of the time! But to predict the number of rabbits at time \\(t\\), we simply need to know the initial abundance (\\(N_0\\)) and the birth rate (\\(b\\)): \\(N_t = N_0(1+b)^t\\)"
  },
  {
    "objectID": "posts/diff_eq1/diff_eq_1.html#discrete-to-continuous",
    "href": "posts/diff_eq1/diff_eq_1.html#discrete-to-continuous",
    "title": "Simulating ecology models: A series of tutorial",
    "section": "Discrete to Continuous",
    "text": "Discrete to Continuous\nNow how does this relate to the continuous model? The secret is that the continuous model is just the discrete model but we’re making the time step smaller and smaller. Here’s a thought experiment, here I’m assuming that the rabbit is expanding day by day. Nothing about this model is stopping me from thinking of growth every… hour, minute, second, or nanosecond. However, if we want to have a model that is growth every 12 hours instead of 24 hours, we need to adjust \\(b\\) by multiplying it by \\(\\frac{1}{2}\\). If it’s every hour, we need to adjust \\(b\\) by multiplying it by \\(\\frac{1}{24}\\). So ultimately, we multiply \\(b\\) by some time step which we call: \\(\\Delta T\\).\nNow how does the step from discrete to continuous happen? Starting from this, we can move one of the N_0 \\[\nN_{t+1} = N_{t} + bN_{t} \\Delta t.\n\\] \\[\nN_{t+1} - N_t =  b N_t \\Delta t\n\\] \\[\n\\frac{N_{t+1} - N_t}{\\Delta t} = {bN_t}\n\\] As you let \\(\\delta T\\) approach infinity, the discrete equation can then be described as a differential equation.\n\\[ \\frac{dN}{dt} = {bN_t} \\] Solving it, we get:\n\\[\nN(t) = N_0exp(b(t))\n\\]\nNext step! How do you numerically solve a differential equation."
  },
  {
    "objectID": "posts/diff_eq1/diff_eq_1.html#the-exponential-model",
    "href": "posts/diff_eq1/diff_eq_1.html#the-exponential-model",
    "title": "Simulating ecology models: A series of tutorial",
    "section": "The exponential model",
    "text": "The exponential model\nHere, the exponential model returns. Wildly popular, wildly unrealistic, and appears in many aspects. It is the intro ecology student’s first introduction to population growth models.\nLet’s say we have a per-capita growth rate of 5 new rabbits per rabbit per time. In other words:\n$$\n$$\nThat means that \\(r = 5\\)\nSo what does that mean? We know that\n\\[\n\\frac{dN}{dt} =\n\\]\nHow to solve it? So there’s an entire textbook of solving differential equations:\n\\[\nN(t) = N_0 e^{r(t)}\n\\] So aha, we found the analytical solution (which honestly would many models especailly as they become more and more complicated are less likely to have a clean solution).\nMost of the time we have to numerically solve it! Which would be the next section!"
  },
  {
    "objectID": "posts/diff_eq1/diff_eq_1.html#discrete-to-continuous-1",
    "href": "posts/diff_eq1/diff_eq_1.html#discrete-to-continuous-1",
    "title": "Simulating ecology models: A series of tutorial",
    "section": "Discrete to continuous",
    "text": "Discrete to continuous\nSo how does the step to discrete to continuous happen?\n\\[ N\\_{t+1} - N_t = \\Delta T \\space b \\space N_t \\]\n\\[\n\\frac{N_{t+1} - N_t}{\\Delta t} = {bN_t}\n\\]\n\\[\\frac{dN}{dt} = {bN_t} \\]"
  },
  {
    "objectID": "posts/briere/simulating_tpc.html",
    "href": "posts/briere/simulating_tpc.html",
    "title": "Quick how to: Simulating different thermal performance curves",
    "section": "",
    "text": "WORK IN PROGRESS\nFor a project, I’m perhaps interested in modeling different thermal performance curves. Using the Briere function, for example, the equation looks like this:\n\\[\naT(T - T_L)(T_U- T)^{1/d}\n\\]\nas an example\nLet’s say T goes from 10 to 30 Celsius. a and d are parameters that are to be estimated.\n\nTemperature_range = seq(0,40)\na = 0.1\nTL = 10\nTU = 35\nd = 2\n\n\neef &lt;- a * Temperature_range * (Temperature_range - TL) * (TU -Temperature_range)^(1/d)\n\n\nplot(Temperature_range, eef)\n\n\n\n\n\n\n\n\n```{r}"
  },
  {
    "objectID": "posts/simulate_scale_free_network/simulatenetwork.html",
    "href": "posts/simulate_scale_free_network/simulatenetwork.html",
    "title": "Simulating a scale-free network",
    "section": "",
    "text": "library(igraph)\nlibrary(ggplot2)\nlibrary(reshape2)\nset.seed(24601)"
  },
  {
    "objectID": "posts/simulate_scale_free_network/simulatenetwork.html#an-introduction",
    "href": "posts/simulate_scale_free_network/simulatenetwork.html#an-introduction",
    "title": "Simulating a scale-free network",
    "section": "An introduction",
    "text": "An introduction\nNote: I have done a much better way (but I have left this up as a historical relic)\nThe goal is to create a spatial network that is scale-free with the user having the ability to vary the connectance. Assuming that \\(n\\) is the number of patches and \\(l\\) is the number of edges,the connectance (\\(C\\)) is then: \\(\\frac{l}{n^2}\\)."
  },
  {
    "objectID": "posts/simulate_scale_free_network/simulatenetwork.html#calculate-the-maximum-connectance-from-the-number-of-nodes-n",
    "href": "posts/simulate_scale_free_network/simulatenetwork.html#calculate-the-maximum-connectance-from-the-number-of-nodes-n",
    "title": "Simulating a scale-free network",
    "section": "Calculate the maximum connectance from the number of nodes, \\(n\\)",
    "text": "Calculate the maximum connectance from the number of nodes, \\(n\\)"
  },
  {
    "objectID": "posts/simulate_scale_free_network/simulatenetwork.html#size-of-the-network-and-connectance",
    "href": "posts/simulate_scale_free_network/simulatenetwork.html#size-of-the-network-and-connectance",
    "title": "Simulating a scale-free network",
    "section": "Size of the network and connectance",
    "text": "Size of the network and connectance\nThe number of patches then determines the minimum and maximum number of edges possible. In an undirected graph, the minimum number of edges we can have is \\(n-1\\) . For the maximum number of edges: \\(n \\frac{(n-1)}{2}\\). With both the number of patches and edges, we can then calculate the minimum and maximum value of connectance.\nFor example, if we have 20 patches (\\(n = 20\\)):\n\nn = 20\nmin_edge = n - 1 # 19\nmax_edge = n * (n-1)/2 # 190 \n\nWith the minimum and maximum number of edges being 19 and 190 respectfully, the minimum and maximum connectances are:\n\nc(min_edge/(n^2), max_edge/(n^2))\n\n[1] 0.0475 0.4750\n\n\nHere is a short function to calculate connectivity when given a network:\n\n###Put an igraph network \ncalculate_connectivity &lt;- function(network){\n  \n  nodes =  vcount(network) #Code the number of vertex/patch/node\n  edges = ecount(network) #Count the number of edges\n  return(edges/(nodes^2)) #Return the connectance\n  \n}\n\nTherefore, with a given number of patches, I can calculate the number of edges needed to get the specific value of connectance."
  },
  {
    "objectID": "posts/simulate_scale_free_network/simulatenetwork.html#first-create-coordinates",
    "href": "posts/simulate_scale_free_network/simulatenetwork.html#first-create-coordinates",
    "title": "Simulating a scale-free network",
    "section": "First, create coordinates",
    "text": "First, create coordinates"
  },
  {
    "objectID": "posts/simulate_scale_free_network/simulatenetwork.html#first-calculate-the-maximum-connectance-from-the-number-of-nodes-n",
    "href": "posts/simulate_scale_free_network/simulatenetwork.html#first-calculate-the-maximum-connectance-from-the-number-of-nodes-n",
    "title": "Simulating a scale-free network",
    "section": "First, calculate the maximum connectance from the number of nodes, \\(n\\)",
    "text": "First, calculate the maximum connectance from the number of nodes, \\(n\\)\n\ncalculate_maximum_connectance &lt;- function(n){\n  max_edges &lt;- n * (n-1)/2 \n\n  return(max_edges/(n^2))\n}\n\nNow what is the lowest number of \\(m\\) does it have to to be for the highest connectance? I test this alot and it seems to be \\(n-1\\), makes sense.\nSo here we can get some ideas:\n\nm_vec &lt;-seq(1,n-1)\nlist_network = NULL\nfor (i in seq(1,length(m_vec))){\n  \n  \n net&lt;- sample_pa(n,directed = FALSE, m = m_vec [[i]], algorithm = \"psumtree\")\n \n \nlist_network[[i]]= data.frame(connectivity = calculate_connectivity(net),\n                     edges =ecount(net))\n\n\n}\ndf_network &lt;- do.call(rbind, list_network)\n\n\nCreate a series of networks with the right connectivity\n\nrand_vect &lt;- function(N, M, sd = 1, pos.only = TRUE) {\n  vec &lt;- rnorm(N, M/N, sd)\n  if (abs(sum(vec)) &lt; 0.01) vec &lt;- vec + 1\n  vec &lt;- round(vec / sum(vec) * M)\n  deviation &lt;- M - sum(vec)\n  for (. in seq_len(abs(deviation))) {\n    vec[i] &lt;- vec[i &lt;- sample(N, 1)] + sign(deviation)\n  }\n  if (pos.only) while (any(vec &lt; 0)) {\n    negs &lt;- vec &lt; 0\n    pos  &lt;- vec &gt; 0\n    vec[negs][i] &lt;- vec[negs][i &lt;- sample(sum(negs), 1)] + 1\n    vec[pos][i]  &lt;- vec[pos ][i &lt;- sample(sum(pos ), 1)] - 1\n  }\n  vec\n}\n\n\ngenOutSeq &lt;- function(n, m) {\n  n &lt;- n-1 # Shift it along\n  rem &lt;- m %% n\n  c(0, rep(m%/%n + 1, rem), rep(m%/%n, n - rem))\n  \n}"
  },
  {
    "objectID": "posts/simulate_scale_free_network/simulatenetwork.html#simulating-creating-spatial-coordinates",
    "href": "posts/simulate_scale_free_network/simulatenetwork.html#simulating-creating-spatial-coordinates",
    "title": "Simulating a scale-free network",
    "section": "Simulating creating spatial coordinates",
    "text": "Simulating creating spatial coordinates\nFirst, we create a spatial network. We sample the coordinates from a uniform distribution with a maximum distance:\n\nx.coordinates &lt;- runif(n,0,10)\ny.coordinates &lt;- runif(n,0,10)\npoints &lt;- cbind(x.coordinates,y.coordinates)\n\n\n\n\n\n\n\n\n\n\nI’m, however, more interested in getting the distances between points.\n\ndistance_matrix &lt;- as.matrix(dist(points))\n\nThis gives us a matrix that has the distances between points."
  },
  {
    "objectID": "posts/simulate_scale_free_network/simulatenetwork.html#for-each-connectance-value-calculate-the-number-of-edges",
    "href": "posts/simulate_scale_free_network/simulatenetwork.html#for-each-connectance-value-calculate-the-number-of-edges",
    "title": "Simulating a scale-free network",
    "section": "For each connectance value, calculate the number of edges",
    "text": "For each connectance value, calculate the number of edges\nThis is a function to calculate the number of edges. I want to get connectance values of 0.05, 0.10, 0.20, 0.30, and 0.40. Then the total number of edges needed based on the patch number is: \\(C n^2\\)\n\ncalculate_edge &lt;- function(n){\n  connectance  = c(0.05,0.10,0.20,0.30,0.40) #good ranges?\n  \n   return(connectance * n^2)\n}\n\nTherefore, if I want to create a network with this connectance value, here are the number of edges that I need:\n\ncbind.data.frame(connectance = c(0.05,0.10, 0.20, 0.30,0.40),\n                 edges = calculate_edge(20))\n\n  connectance edges\n1        0.05    20\n2        0.10    40\n3        0.20    80\n4        0.30   120\n5        0.40   160"
  },
  {
    "objectID": "posts/simulate_scale_free_network/simulatenetwork.html#using-barabásialbert-for-preferential-attachment",
    "href": "posts/simulate_scale_free_network/simulatenetwork.html#using-barabásialbert-for-preferential-attachment",
    "title": "Simulating a scale-free network",
    "section": "Using Barabási–Albert for preferential attachment",
    "text": "Using Barabási–Albert for preferential attachment\nI’m assuming that the spatial network is scale-free (Need to find more references to verify this). I don’t think this is a bad assumption. In igraph, to create a scale-free network with preferential attachment, we uses sample_pa. In this stochastic algorithm, you add new nodes and edges with each time step.\nThe only way to manipulate the total number of edges you want is by using the “out_seq” argment which states: Numeric vector giving the number of edges to add in each time step. Its first element is ignored as no edges are added in the first time step”. Therefore, you can ensure that you have the right number of edges added by summing the vector.\nHere, is a function that lets me generate a number of edges to be added with each time step (I took code from this stackoverflow post) (Source: Generate Random network models with specified number of edges).\n\ngenOutSeq &lt;- function(n, m) {\n  n &lt;- n-1 # Shift it along\n  rem &lt;- m %% n\n  c(0, rep(m%/%n + 1, rem), rep(m%/%n, n - rem))\n  \n}\n\n\n### The out_seq to be put into the network.\nedges_list &lt;-  lapply(calculate_edge (20),  function(x) genOutSeq(n,x))\n\n\nadjacency_matrix_list &lt;- list()\n\nfor (k in seq(1,length(edges_list))){\n\nnet &lt;- sample_pa(\n  n, #Number of patches\n  power = 1,\n  out.seq = edges_list[[k]], #This is the vector list\n  zero.appeal = 1,\n  directed = FALSE, #undirected\n  algorithm = c(\"psumtree\"), #Prevents multiedges\n  start.graph = NULL)\n\n\n###It is possible that I don't have the right number of edges\n###so I must randomly add edges if there are some missing\nnMulti &lt;- sum(edges_list[[k]]) - gsize(net)\n\n\n### We basically run this until we have the correct number of edges AND\n### there are no mutliedges\nwhile(is_simple(net) == FALSE){\nfor (i in 1:nMulti) {\n  vPair &lt;- sample(1:n, size = 2)\n  net &lt;- add_edges(net, vPair)\n  \n  net &lt;- simplify(net,\n  remove.multiple = TRUE,\n  remove.loops = TRUE)\n}\n}\n\n###Convert graph to adjacency matrix\nadj_matrix &lt;- as_adjacency_matrix(\n  net,\n  type = c(\"both\"),\nsparse = \"false\")\n\n### Save adjacency matrix ot a list\nadjacency_matrix_list[[k]] &lt;- as.matrix(adj_matrix)\n}"
  },
  {
    "objectID": "posts/simulate_scale_free_network/simulatenetwork.html#combining-the-spatial-distances-and-the-network",
    "href": "posts/simulate_scale_free_network/simulatenetwork.html#combining-the-spatial-distances-and-the-network",
    "title": "Simulating a scale-free network",
    "section": "Combining the spatial distances and the network",
    "text": "Combining the spatial distances and the network\nTaking the idea from the previous section, I realized that I should simulate the same network with various distances between the patches.\n\ngenerate_distance_matrix &lt;- function(n, max_distance){\n  x.coordinates &lt;- runif(n,0,max_distance)\n  y.coordinates &lt;- runif(n,0,max_distance)\n  points &lt;- cbind(x.coordinates,y.coordinates)\n  distance_matrix &lt;- as.matrix(dist(points))\n  \n  return(distance_matrix)\n  \n  \n}\ngenerate_distance_matrix(20,16)\n\n            1          2         3         4         5          6          7\n1   0.0000000  8.6175165 11.655109  9.112391  6.404841  5.9089007  6.0997858\n2   8.6175165  0.0000000 10.990153 14.130475 13.908119  6.3941861  2.7532546\n3  11.6551087 10.9901532  0.000000  7.820306 11.376322 14.8731465  9.5424053\n4   9.1123912 14.1304746  7.820306  0.000000  4.730953 14.6410196 11.5533601\n5   6.4048412 13.9081193 11.376322  4.730953  0.000000 12.3096419 11.1552559\n6   5.9089007  6.3941861 14.873147 14.641020 12.309642  0.0000000  5.7051202\n7   6.0997858  2.7532546  9.542405 11.553360 11.155256  5.7051202  0.0000000\n8  10.0615160 11.8957354  3.261227  4.563092  8.419182 14.3881948  9.8094469\n9   6.3298472  5.7468549 14.667616 14.844755 12.730002  0.7722736  5.3193032\n10  1.5206241 10.1114869 12.131785  8.522742  5.200992  7.1917517  7.5432196\n11  4.8569345 13.4652417 14.206410  8.732522  4.161066 10.0078166 10.8847605\n12 14.2192858 15.9176823  5.346523  6.586582 11.282209 18.7242766 14.0282542\n13  5.5689979  6.5125511 14.728153 14.347009 11.966004  0.3603898  5.6672092\n14  3.7912128 11.6638534 11.147016  6.172511  2.616587  9.6932863  8.9368108\n15  8.1779382  0.5785997 10.506733 13.551950 13.367012  6.3453653  2.2160576\n16  0.7136292  9.0261390 12.364270  9.598716  6.589343  5.7670339  6.5971229\n17  6.5508088  2.4267505  9.421134 11.760379 11.510713  6.0288071  0.4617598\n18  6.9659857  3.3330187  7.988477 10.887960 11.188305  7.3829891  1.6842108\n19 10.5529303  9.0708803  1.935084  8.455247 11.322822 13.2246967  7.7493002\n20  8.7900665  7.9959965  3.269103  7.709081  9.974725 11.6095843  6.3086001\n           8          9        10        11        12         13        14\n1  10.061516  6.3298472  1.520624  4.856934 14.219286  5.5689979  3.791213\n2  11.895735  5.7468549 10.111487 13.465242 15.917682  6.5125511 11.663853\n3   3.261227 14.6676164 12.131785 14.206410  5.346523 14.7281532 11.147016\n4   4.563092 14.8447547  8.522742  8.732522  6.586582 14.3470087  6.172511\n5   8.419182 12.7300016  5.200992  4.161066 11.282209 11.9660036  2.616587\n6  14.388195  0.7722736  7.191752 10.007817 18.724277  0.3603898  9.693286\n7   9.809447  5.3193032  7.543220 10.884761 14.028254  5.6672092  8.936811\n8   0.000000 14.3520920 10.166707 11.685433  4.336181 14.1741402  8.658880\n9  14.352092  0.0000000  7.682566 10.606512 18.684757  1.0699966 10.120062\n10 10.166707  7.6825665  0.000000  3.353893 14.134935  6.8391626  2.624882\n11 11.685433 10.6065121  3.353893  0.000000 15.105187  9.6474534  3.061283\n12  4.336181 18.6847568 14.134935 15.105187  0.000000 18.5090168 12.250093\n13 14.174140  1.0699966  6.839163  9.647453 18.509017  0.0000000  9.349431\n14  8.658880 10.1200616  2.624882  3.061283 12.250093  9.3494305  0.000000\n15 11.342895  5.7411839  9.659216 13.012267 15.388788  6.4328025 11.148412\n16 10.727791  6.2603784  1.425149  4.558631 14.855356  5.4149740  3.983245\n17  9.842589  5.6026458  7.986216 11.323122 14.018737  6.0105001  9.329770\n18  8.587498  7.0031835  8.280557 11.530830 12.687169  7.3344379  9.230654\n19  4.194378 12.9678057 11.234096 13.657904  7.185773 13.1045056 10.649714\n20  4.114967 11.3985156  9.503237 12.029807  7.923987 11.4704602  9.065163\n           15         16         17        18        19        20\n1   8.1779382  0.7136292  6.5508088  6.965986 10.552930  8.790067\n2   0.5785997  9.0261390  2.4267505  3.333019  9.070880  7.995997\n3  10.5067329 12.3642701  9.4211339  7.988477  1.935084  3.269103\n4  13.5519503  9.5987161 11.7603787 10.887960  8.455247  7.709081\n5  13.3670124  6.5893427 11.5107129 11.188305 11.322822  9.974725\n6   6.3453653  5.7670339  6.0288071  7.382989 13.224697 11.609584\n7   2.2160576  6.5971229  0.4617598  1.684211  7.749300  6.308600\n8  11.3428955 10.7277914  9.8425893  8.587498  4.194378  4.114967\n9   5.7411839  6.2603784  5.6026458  7.003183 12.967806 11.398516\n10  9.6592164  1.4251494  7.9862156  8.280557 11.234096  9.503237\n11 13.0122669  4.5586309 11.3231218 11.530830 13.657904 12.029807\n12 15.3887880 14.8553558 14.0187369 12.687169  7.185773  7.923987\n13  6.4328025  5.4149740  6.0105001  7.334438 13.104506 11.470460\n14 11.1484123  3.9832452  9.3297702  9.230654 10.649714  9.065163\n15  0.0000000  8.6127032  1.8652457  2.766816  8.596757  7.473197\n16  8.6127032  0.0000000  7.0544556  7.558628 11.266294  9.503158\n17  1.8652457  7.0544556  0.0000000  1.452262  7.592865  6.220783\n18  2.7668161  7.5586280  1.4522615  0.000000  6.145836  4.821852\n19  8.5967575 11.2662935  7.5928649  6.145836  0.000000  1.766750\n20  7.4731970  9.5031579  6.2207827  4.821852  1.766750  0.000000\n\n\n\n### Generate a 5 distance matrix\n\ndistance_matrices&lt;- replicate(5,generate_distance_matrix(20,16),simplify=FALSE)\n\n\nspatial_scale_free_network &lt;- NULL\n\n\nfor (i in seq(1,length(adjacency_matrix_list))){\n  \n  adjacency_matrix_list_Interest &lt;-  adjacency_matrix_list [[i]] \n  \n  spatial_scale_free_network [[i]] &lt;- lapply(distance_matrices, function(x) x *  adjacency_matrix_list_Interest)\n        \n\n}\n\nLet’s plot it out:\n\ngraph_low_connectance&lt;-   lapply(\nspatial_scale_free_network[[1]],function(x)\ngraph_from_adjacency_matrix(x, mode = \n                            \"undirected\", weighted= \"TRUE\"))\n\n\npar(mfrow=c(3,2))\n\nfor (i in seq(1,5)){\n  plot(graph_low_connectance[[i]])\n}\n\n\n\n\n\n\n\n\n}"
  },
  {
    "objectID": "posts/simulate_scale_free_network/simulatenetwork.html#combining-the-spatial-distances-and-the-scale-free-network",
    "href": "posts/simulate_scale_free_network/simulatenetwork.html#combining-the-spatial-distances-and-the-scale-free-network",
    "title": "Simulating a scale-free network",
    "section": "Combining the spatial distances and the scale-free network",
    "text": "Combining the spatial distances and the scale-free network\nI realized that I should simulate the same network with various distances between the patches.\n\n###This generates the distance matrices between patches\ngenerate_distance_matrix &lt;- function(n, max_distance){\n  x.coordinates &lt;- runif(n,0,max_distance)\n  y.coordinates &lt;- runif(n,0,max_distance)\n  points &lt;- cbind(x.coordinates,y.coordinates)\n  distance_matrix &lt;- as.matrix(dist(points))\n  \n  return(distance_matrix)\n  \n}\ngenerate_distance_matrix(20,16)\n\n           1         2         3         4         5          6         7\n1   0.000000  5.157415 15.193534  6.956727 10.595116 11.0874552 11.626523\n2   5.157415  0.000000 12.584992  2.113439  7.126970 13.1673740  9.468252\n3  15.193534 12.584992  0.000000 10.849847  5.601309 11.9520194  3.604385\n4   6.956727  2.113439 10.849847  0.000000  5.273883 13.2341047  8.008133\n5  10.595116  7.126970  5.601309  5.273883  0.000000 11.7723962  3.387007\n6  11.087455 13.167374 11.952019 13.234105 11.772396  0.0000000  9.433731\n7  11.626523  9.468252  3.604385  8.008133  3.387007  9.4337309  0.000000\n8   4.873067  2.974354 10.609897  3.173989  5.727345 10.2786557  7.181685\n9  14.337942 10.288828  5.060660  8.208842  3.892680 14.9467973  5.563506\n10  9.978614  4.826225 11.473110  3.284642  6.138704 16.1859300  9.464290\n11  2.330038  4.518944 12.907996  5.762383  8.536265  9.4506963  9.325471\n12  7.855760  5.708047  7.382344  4.647664  3.186196  9.2709702  3.922495\n13  5.868926  2.873714  9.966747  2.343386  4.823247 10.8959945  6.683709\n14 13.941346 14.882039  9.894034 14.411544 11.511777  3.8356737  8.440559\n15 11.518690 13.523889 11.903111 13.544006 11.931739  0.4457247  9.495183\n16 12.320441 10.815616  3.780814  9.545150  5.198992  8.2663361  1.822419\n17 10.769585  8.458042  4.425673  6.997295  2.596253  9.5504748  1.014959\n18 13.825584 15.428669 11.728347 15.210645 12.855849  2.8619062  9.980735\n19  6.031332  4.818469  9.162611  4.490833  4.960311  8.7478925  5.609609\n20 10.288681 11.010526  8.654084 10.636661  8.434724  3.4550483  5.979122\n           8         9        10        11        12        13        14\n1   4.873067 14.337942  9.978614  2.330038  7.855760  5.868926 13.941346\n2   2.974354 10.288828  4.826225  4.518944  5.708047  2.873714 14.882039\n3  10.609897  5.060660 11.473110 12.907996  7.382344  9.966747  9.894034\n4   3.173989  8.208842  3.284642  5.762383  4.647664  2.343386 14.411544\n5   5.727345  3.892680  6.138704  8.536265  3.186196  4.823247 11.511777\n6  10.278656 14.946797 16.185930  9.450696  9.270970 10.895994  3.835674\n7   7.181685  5.563506  9.464290  9.325471  3.922495  6.683709  8.440559\n8   0.000000  9.475056  6.432780  2.983490  3.259538  1.101142 11.909801\n9   9.475056  0.000000  7.503353 12.369914  7.064457  8.475221 13.900773\n10  6.432780  7.503353  0.000000  9.016932  7.072227  5.493291 16.923081\n11  2.983490 12.369914  9.016932  0.000000  5.631318  4.078112 11.958847\n12  3.259538  7.064457  7.072227  5.631318  0.000000  2.843700  9.879585\n13  1.101142  8.475221  5.493291  4.078112  2.843700  0.000000 12.214657\n14 11.909801 13.900773 16.923081 11.958847  9.879585 12.214657  0.000000\n15 10.619311 15.032079 16.462386  9.858851  9.506757 11.210803  3.471485\n16  8.265301  7.118348 11.224512  9.990403  5.113260  7.956910  6.782439\n17  6.227417  5.421074  8.549443  8.492820  2.973592  5.688215  8.954794\n18 12.468831 15.518238 17.932983 12.048341 10.864687 12.926205  1.934995\n19  1.896866  8.852179  7.497992  3.768494  1.878649  2.170724 10.088460\n20  8.036498 11.497423 13.321285  8.190764  6.254737  8.388567  3.880907\n           15        16        17        18        19        20\n1  11.5186899 12.320441 10.769585 13.825584  6.031332 10.288681\n2  13.5238889 10.815616  8.458042 15.428669  4.818469 11.010526\n3  11.9031106  3.780814  4.425673 11.728347  9.162611  8.654084\n4  13.5440064  9.545150  6.997295 15.210645  4.490833 10.636661\n5  11.9317388  5.198992  2.596253 12.855849  4.960311  8.434724\n6   0.4457247  8.266336  9.550475  2.861906  8.747892  3.455048\n7   9.4951835  1.822419  1.014959  9.980735  5.609609  5.979122\n8  10.6193112  8.265301  6.227417 12.468831  1.896866  8.036498\n9  15.0320794  7.118348  5.421074 15.518238  8.852179 11.497423\n10 16.4623864 11.224512  8.549443 17.932983  7.497992 13.321285\n11  9.8588514  9.990403  8.492820 12.048341  3.768494  8.190764\n12  9.5067573  5.113260  2.973592 10.864687  1.878649  6.254737\n13 11.2108027  7.956910  5.688215 12.926205  2.170724  8.388567\n14  3.4714848  6.782439  8.954794  1.934995 10.088460  3.880907\n15  0.0000000  8.258882  9.658070  2.417472  9.053737  3.539507\n16  8.2588825  0.000000  2.675237  8.435509  6.495927  4.897432\n17  9.6580700  2.675237  0.000000 10.374968  4.738255  6.119064\n18  2.4174721  8.435509 10.374968  0.000000 10.757652  4.611757\n19  9.0537368  6.495927  4.738255 10.757652  0.000000  6.238836\n20  3.5395065  4.897432  6.119064  4.611757  6.238836  0.000000\n\n\n\n### Generate 5 distance matrix - ideally i should have a large number of simulated distance matrices (n = 1000)\ndistance_matrices&lt;- replicate(5,generate_distance_matrix(20,16),simplify=FALSE)\n\nThis code is to basically take the adjacency matrix that we created earlier with varying connectance and multiply it by the distance matrix.\n\nspatial_scale_free_network &lt;- NULL\n\nfor (i in seq(1, length(adjacency_matrix_list))) {\n  adjacency_matrix_list_Interest &lt;- adjacency_matrix_list[[i]]\n\n  spatial_scale_free_network[[i]] &lt;- lapply(distance_matrices, function(x) x * adjacency_matrix_list_Interest)\n}\n\nLet’s plot it out. Here we are looking at the adjaceny matrix associated with low connectance.\n\ngraph_low_connectance&lt;-   lapply(\nspatial_scale_free_network[[1]],function(x)\ngraph_from_adjacency_matrix(x, mode = \n                            \"undirected\", \n                            weighted= \"TRUE\"))\n\nOk, you should be able to see different distances between patches.\n\npar(mfrow=c(3,2))\nfor (i in seq(1,5)){\n  plot(graph_low_connectance[[i]],layout=layout.auto)\n}\n\n\n\n\n\n\n\n\nHere is a high connectance one:\n\ngraph_high_connectance&lt;-   lapply(\nspatial_scale_free_network[[5]],function(x)\ngraph_from_adjacency_matrix(x, mode = \n                            \"undirected\", weighted= \"TRUE\"))\n\n\npar(mfrow=c(3,2))\n\nfor (i in seq(1,5)){\n  plot(graph_high_connectance[[i]],layout=layout.auto)\n}"
  },
  {
    "objectID": "posts/interesting_paper_week2/interesting_paperweek2.html",
    "href": "posts/interesting_paper_week2/interesting_paperweek2.html",
    "title": "Interesting stuff I read (August 19th)",
    "section": "",
    "text": "Hereford 2017\nInteresting paper where I learned about MSDM (mechanical species distribution modeling). Mostly experimental, but the paper explores the thermal performance curves (TPCs) of differing genotype in Mollugo verticillata in California. Interestingly, they found that thermal breadth is negatively correlated with optimum temperature. Being more fit in warmer temperature comes at a cost of breadth. A question that I am curious about : is variation in TPC adaptive?"
  },
  {
    "objectID": "posts/interesting_stuff_I_learned/Untitled.html",
    "href": "posts/interesting_stuff_I_learned/Untitled.html",
    "title": "Interesting things I learned this week (August 19th)",
    "section": "",
    "text": "This is a list of things I learned. Generally involves my research, but you might see a weird aside."
  },
  {
    "objectID": "posts/interesting_stuff_I_learned/Untitled.html#i-learned-that-its-michelle-pfeiffer-who-played-ambers-mom-in-the-movie-adaptation-of-hairspray-2007",
    "href": "posts/interesting_stuff_I_learned/Untitled.html#i-learned-that-its-michelle-pfeiffer-who-played-ambers-mom-in-the-movie-adaptation-of-hairspray-2007",
    "title": "Interesting things I learned this week (August 19th)",
    "section": "I learned that it’s Michelle Pfeiffer who played Amber’s mom in the movie adaptation of Hairspray (2007)",
    "text": "I learned that it’s Michelle Pfeiffer who played Amber’s mom in the movie adaptation of Hairspray (2007)\nAnd not Hannah Waddingham? I do not feel bad about this mistake- look it up!"
  },
  {
    "objectID": "posts/interesting_stuff_I_learned/Untitled.html#profvis-is-a-cool-package-that-helps-you-visualize-profiling-your-code",
    "href": "posts/interesting_stuff_I_learned/Untitled.html#profvis-is-a-cool-package-that-helps-you-visualize-profiling-your-code",
    "title": "Interesting things I learned this week (August 19th)",
    "section": "profvis is a cool package that helps you visualize profiling your code",
    "text": "profvis is a cool package that helps you visualize profiling your code\nAnd figure out where the slowdowns. I learned that the slowdowns are not exactly where I thought they would be?"
  },
  {
    "objectID": "posts/interesting_stuff_I_learned/Untitled.html#you-can-save-some-miliseconds-using-rep_length-instead-of-rep",
    "href": "posts/interesting_stuff_I_learned/Untitled.html#you-can-save-some-miliseconds-using-rep_length-instead-of-rep",
    "title": "Interesting things I learned this week (August 19th)",
    "section": "You can save some miliseconds using rep_length instead of rep",
    "text": "You can save some miliseconds using rep_length instead of rep"
  },
  {
    "objectID": "posts/interesting_stuff_I_learned/Untitled.html#apply-can-be-slow",
    "href": "posts/interesting_stuff_I_learned/Untitled.html#apply-can-be-slow",
    "title": "Interesting things I learned this week (August 19th)",
    "section": "Apply can be slow",
    "text": "Apply can be slow\nI’m still trying to figure this out."
  },
  {
    "objectID": "posts/interesting_stuff_I_learned/Untitled.html#save-some-miliseconds-using-rep_length-instead-of-rep",
    "href": "posts/interesting_stuff_I_learned/Untitled.html#save-some-miliseconds-using-rep_length-instead-of-rep",
    "title": "Interesting things I learned this week (August 19th)",
    "section": "Save some miliseconds using rep_length instead of rep",
    "text": "Save some miliseconds using rep_length instead of rep\nSame function, but somehow it saves you a few seconds?"
  },
  {
    "objectID": "posts/interesting_stuff_I_learned/Untitled.html#you-can-create-sparse-matrix-using-the-package-matrix",
    "href": "posts/interesting_stuff_I_learned/Untitled.html#you-can-create-sparse-matrix-using-the-package-matrix",
    "title": "Interesting things I learned this week (August 19th)",
    "section": "You can create sparse Matrix using the package Matrix",
    "text": "You can create sparse Matrix using the package Matrix\nI didn’t realize how much a dense matrix that is only 20 x 20 can still be slow?"
  },
  {
    "objectID": "posts/interesting_stuff_I_learned/Untitled.html#michelle-pfeiffer-plays-ambers-mom-in-the-movie-adaptation-of-hairspray-2007",
    "href": "posts/interesting_stuff_I_learned/Untitled.html#michelle-pfeiffer-plays-ambers-mom-in-the-movie-adaptation-of-hairspray-2007",
    "title": "Interesting things I learned this week (August 19th)",
    "section": "Michelle Pfeiffer plays Amber’s mom in the movie adaptation of Hairspray (2007)",
    "text": "Michelle Pfeiffer plays Amber’s mom in the movie adaptation of Hairspray (2007)\nAnd not Hannah Waddingham. I do not feel bad about this mistake- look it up!"
  },
  {
    "objectID": "posts/interesting_paper_week2/interesting_paperweek2.html#climate-sensitivity-across-latitude-scaling-physiology-to-communities",
    "href": "posts/interesting_paper_week2/interesting_paperweek2.html#climate-sensitivity-across-latitude-scaling-physiology-to-communities",
    "title": "Interesting stuff I read (August 19th)",
    "section": "Climate sensitivity across latitude: scaling physiology to communities",
    "text": "Climate sensitivity across latitude: scaling physiology to communities\nLouthan et al. 2021\nA nice review paper that I really love. It talks about TPCs and how they affect individuals and communities. For example, shapes of TPCs can vary across latitudes due to trade-offs. This can then scale up to affect the community and influence stability. Don’t let the short paragraph deter you, I really recommend."
  },
  {
    "objectID": "posts/interesting_paper_week2/interesting_paperweek2.html#the-failure-of-r_0",
    "href": "posts/interesting_paper_week2/interesting_paperweek2.html#the-failure-of-r_0",
    "title": "Interesting stuff I read (August 19th)",
    "section": "The Failure of \\(R_0\\)",
    "text": "The Failure of \\(R_0\\)\nLi et al. 2001\nA bit of a concerning and eye-opening paper about how \\(R_0\\) is kinda nebulous and that different methods of calculating \\(R_0\\) lead to varying answers (Wild to me!). Talks about how \\(R_0\\) is flawed, but is the best we got. Interesting paper, points out some of the critical failures in the next generation method and such."
  },
  {
    "objectID": "posts/interesting_stuff_I_learned/stuffilearnedpart1.html",
    "href": "posts/interesting_stuff_I_learned/stuffilearnedpart1.html",
    "title": "Interesting things I learned this week (August 19th)",
    "section": "",
    "text": "This is a list of things I learned. Generally involves my research, but you might see a weird aside."
  },
  {
    "objectID": "posts/interesting_stuff_I_learned/stuffilearnedpart1.html#profvis-is-a-cool-package-that-helps-you-visualize-profiling-your-code",
    "href": "posts/interesting_stuff_I_learned/stuffilearnedpart1.html#profvis-is-a-cool-package-that-helps-you-visualize-profiling-your-code",
    "title": "Interesting things I learned this week (August 19th)",
    "section": "profvis is a cool package that helps you visualize profiling your code",
    "text": "profvis is a cool package that helps you visualize profiling your code\nAnd figure out where the slowdowns. I learned that the slowdowns are not exactly where I thought they would be?"
  },
  {
    "objectID": "posts/interesting_stuff_I_learned/stuffilearnedpart1.html#save-some-miliseconds-using-rep_length-instead-of-rep",
    "href": "posts/interesting_stuff_I_learned/stuffilearnedpart1.html#save-some-miliseconds-using-rep_length-instead-of-rep",
    "title": "Interesting things I learned this week (August 19th)",
    "section": "Save some miliseconds using rep_length instead of rep",
    "text": "Save some miliseconds using rep_length instead of rep\nSame function, but somehow it saves you a few seconds?"
  },
  {
    "objectID": "posts/interesting_stuff_I_learned/stuffilearnedpart1.html#apply-can-be-slow",
    "href": "posts/interesting_stuff_I_learned/stuffilearnedpart1.html#apply-can-be-slow",
    "title": "Interesting things I learned this week (August 19th)",
    "section": "Apply can be slow",
    "text": "Apply can be slow\nI’m still trying to figure this out."
  },
  {
    "objectID": "posts/interesting_stuff_I_learned/stuffilearnedpart1.html#you-can-create-sparse-matrix-using-the-package-matrix",
    "href": "posts/interesting_stuff_I_learned/stuffilearnedpart1.html#you-can-create-sparse-matrix-using-the-package-matrix",
    "title": "Interesting things I learned this week (August 19th)",
    "section": "You can create sparse Matrix using the package Matrix",
    "text": "You can create sparse Matrix using the package Matrix\nI didn’t realize how much a dense matrix that is only 20 x 20 can still be slow?"
  },
  {
    "objectID": "posts/interesting_stuff_I_learned/stuffilearnedpart1.html#michelle-pfeiffer-plays-ambers-mom-in-the-movie-adaptation-of-hairspray-2007",
    "href": "posts/interesting_stuff_I_learned/stuffilearnedpart1.html#michelle-pfeiffer-plays-ambers-mom-in-the-movie-adaptation-of-hairspray-2007",
    "title": "Interesting things I learned this week (August 19th)",
    "section": "Michelle Pfeiffer plays Amber’s mom in the movie adaptation of Hairspray (2007)",
    "text": "Michelle Pfeiffer plays Amber’s mom in the movie adaptation of Hairspray (2007)\nAnd not Hannah Waddingham. I do not feel bad about this mistake- look it up!"
  },
  {
    "objectID": "posts/diff_eq1/diff_eq_1.html#coding-it-up",
    "href": "posts/diff_eq1/diff_eq_1.html#coding-it-up",
    "title": "Simulating ecology models: A series of tutorial",
    "section": "Coding it up",
    "text": "Coding it up\nLet’s write a simple code. Please skip to the next section if you are comfortable with creating R functions. Hopefully, my little code will help transition readers into using desolve. Lets go step by step. Personally, I think beginners should start with learning functions as quickly as possible.\nSo how do you design a function for doing mathematical simulations? First, what do you want for your inputs? What are the parts of the model you want to vary? Well, time seems to make the most sense. Maybe I’m interested in seeing what happens in a week… or a thousand years later. The initial rabbit population (\\(N_0\\)) can also vary as well as the birth rate \\(b\\)! So for the input of the function, you should include the parameters that you’re interested in varying.\n\nrabbit_growth &lt;- function(time, initial, b){\n  \n  ###Insert model here\n}\n\nSo I have a function called rabbit_growth and it takes as inputs: time, initial value, and the birth rate. By feeding different values in, we should be getting different values out!\nNow what do we want for the output? For simulation outputs, we want both the time and the abundance at that time. So ‘time’ is the number of days we are interested in projecting the model out to. Let’s say we want 20 days. We can then create a vector called timesteps that goes from 0 to 20, in an increment of a day. Now we have the equation with the initial value and the birth rate (\\(b\\)). We can raise it to the power of the timesteps. So \\(N_0 * (1+b)^t\\) will you give a single numeric value. But multiplying it by a vector, R is smart to know that you want to multiply \\(N_0 *(1+b)\\) to each element of the vector. For example:\n\ntimesteps = seq(0,20)\nprint(timesteps)\n\n [1]  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n\n\n\nprint(paste(\"Single number:\", 5 * (1+5))) # single numeric number #vector\n\n[1] \"Single number: 30\"\n\n\n\n5 * (1+5)^(timesteps) # a vector\n\n [1] 5.000000e+00 3.000000e+01 1.800000e+02 1.080000e+03 6.480000e+03\n [6] 3.888000e+04 2.332800e+05 1.399680e+06 8.398080e+06 5.038848e+07\n[11] 3.023309e+08 1.813985e+09 1.088391e+10 6.530347e+10 3.918208e+11\n[16] 2.350925e+12 1.410555e+13 8.463330e+13 5.077998e+14 3.046799e+15\n[21] 1.828079e+16\n\n\nPutting it all together, here is the completed functino.\n\nrabbit_growth &lt;- function(time, initial, b){\n\n  timesteps =  seq(0,time)\n  \n  abundance  = initial * (1+b)^(timesteps)\n    \n  ###Try to keep the parameter that you are varying as a column for easier analysis \n  rabbit_df &lt;- data.frame(time = timesteps, abundance = abundance,\n                          initial = initial, b = b)\n    \n  return(rabbit_df)\n  }\n\nMy output is now a data.frame with the time, abundance, initial value, and birth rate.\nThere are many, many ways to solve this. Some people do for-loops! If you want to do it using for-loop, this is a perfect exercise for it! Now let’s plot it.\n\nstandard_rabbit &lt;- rabbit_growth(10,5, 1)\nprint(standard_rabbit)\n\n   time abundance initial b\n1     0         5       5 1\n2     1        10       5 1\n3     2        20       5 1\n4     3        40       5 1\n5     4        80       5 1\n6     5       160       5 1\n7     6       320       5 1\n8     7       640       5 1\n9     8      1280       5 1\n10    9      2560       5 1\n11   10      5120       5 1\n\n\nI’m a ggplot lover, so we’re going to use ggplot2:\n\nggplot(standard_rabbit, aes(x = time, y = abundance))+ geom_point() + geom_line() + xlab(\"Time\") + ylab(\"Abundance\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\nNow what if we want to create a lot of different plots? So there is a faster way, but let’s do this for right now. Keeping everything else the same, let’s change \\(b\\) (the third input in the function “rabbit_growth”) and combine these outputs to create one big data frame.\n\nstandard_rabbit1 &lt;- rabbit_growth(10,5, 1)\nstandard_rabbit2 &lt;- rabbit_growth(10,5, 2)\nstandard_rabbit3 &lt;- rabbit_growth(10,5, 3)\n\nstandard_rabbit_all &lt;- rbind(standard_rabbit1,standard_rabbit2, \n                             standard_rabbit3)\n\n\nggplot(standard_rabbit_all, aes(x = time, y= abundance, color = as.factor(b), group = as.factor(b))) + geom_point() + geom_line() +\n  xlab(\"Time\") + ylab(\"Abundance\")+\n  scale_color_viridis(discrete = TRUE, name = \"b\",option = 'turbo')+\n  theme_minimal()\n\n\n\n\n\n\n\n\nWow you can see how drastically the population can grow depending on b!\nCongratulations, you made your first simulation using one of the oldest model of discrete growth!"
  },
  {
    "objectID": "posts/interesting_stuff_I_learned_part2/interestingstuffilearned2.html",
    "href": "posts/interesting_stuff_I_learned_part2/interestingstuffilearned2.html",
    "title": "Interesting things I learned this week (August 26th)",
    "section": "",
    "text": "This is a list of things I learned. Generally involves my research, but you might see a weird aside."
  },
  {
    "objectID": "posts/interesting_stuff_I_learned_part2/interestingstuffilearned2.html#there-is-a-website-with-english-translation-of-17th-century-mathematical-books-newton-and-euler-and-etc.",
    "href": "posts/interesting_stuff_I_learned_part2/interestingstuffilearned2.html#there-is-a-website-with-english-translation-of-17th-century-mathematical-books-newton-and-euler-and-etc.",
    "title": "Interesting things I learned this week (August 26th)",
    "section": "There is a website with english translation of 17th century mathematical books (Newton and Euler and etc.)",
    "text": "There is a website with english translation of 17th century mathematical books (Newton and Euler and etc.)\nhttp://www.17centurymaths.com"
  },
  {
    "objectID": "posts/interesting_stuff_I_learned_part2/interestingstuffilearned2.html#a-stochastic-sir-model-is-implicitly-on-a-network",
    "href": "posts/interesting_stuff_I_learned_part2/interestingstuffilearned2.html#a-stochastic-sir-model-is-implicitly-on-a-network",
    "title": "Interesting things I learned this week (August 26th)",
    "section": "A stochastic SIR model is implicitly on a network",
    "text": "A stochastic SIR model is implicitly on a network\nI mean intuitive, but never thought of it that way before."
  },
  {
    "objectID": "posts/modelingtriatomine/Untitled.html",
    "href": "posts/modelingtriatomine/Untitled.html",
    "title": "Modeling flour beetles",
    "section": "",
    "text": "Assume, we have two classes which I deem the juvenile and the adults.\n\nmain_dat &lt;- read.csv(\"flour.csv\")"
  },
  {
    "objectID": "posts/modelingtriatomine/flour.html",
    "href": "posts/modelingtriatomine/flour.html",
    "title": "Modeling flour beetles",
    "section": "",
    "text": "WORk IN PROGRESS\n\nlibrary(ggplot2)\n\n\nmain_dat &lt;- read.csv(\"flour.csv\",sep=\",\")\n\nIs the individual weight different\n\nmain_dat$weight_ind &lt;- (as.numeric(main_dat$Total.Weight..g.))/as.numeric(main_dat$Adults.Present)\n\nWarning: NAs introduced by coercion\n\n\n\nggplot(main_dat, aes(x = Treatment ,y=  as.numeric(log(as.numeric(main_dat$Adults.Present))),\n                     color = Treatment)) + geom_point(size = 3) +\n  facet_wrap(~Generation)\n\nWarning: Use of `main_dat$Adults.Present` is discouraged.\nℹ Use `Adults.Present` instead.\n\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "posts/stochastic_sis/stochastic_sis.html",
    "href": "posts/stochastic_sis/stochastic_sis.html",
    "title": "Stochastic SIS model",
    "section": "",
    "text": "library(igraph)\nlibrary(ggplot2)\nlibrary(ggnetwork)\nlibrary(dplyr)\nlibrary(gganimate)\n\nThis is for my sake of actually learning how to make a stochastic SIS model."
  },
  {
    "objectID": "posts/interesting_stuff_I_learned_part2/interestingstuffilearned2.html#a-stochastic-sissir-model-is-implicitly-on-a-network",
    "href": "posts/interesting_stuff_I_learned_part2/interestingstuffilearned2.html#a-stochastic-sissir-model-is-implicitly-on-a-network",
    "title": "Interesting things I learned this week (August 26th)",
    "section": "A stochastic SIS/SIR model is implicitly on a network",
    "text": "A stochastic SIS/SIR model is implicitly on a network\nI mean intuitive, but never thought of it that way before."
  },
  {
    "objectID": "posts/interesting_stuff_I_learned_part2/interestingstuffilearned2.html#no-one-can-explain-stochastic-sis-model-well",
    "href": "posts/interesting_stuff_I_learned_part2/interestingstuffilearned2.html#no-one-can-explain-stochastic-sis-model-well",
    "title": "Interesting things I learned this week (August 26th)",
    "section": "No one can explain stochastic SIS model well",
    "text": "No one can explain stochastic SIS model well\nExcept for this guy: https://www.youtube.com/watch?v=sr-DdOnkpro"
  },
  {
    "objectID": "posts/interesting_stuff_I_learned_part2/interestingstuffilearned2.html#inverse-transform-sampling",
    "href": "posts/interesting_stuff_I_learned_part2/interestingstuffilearned2.html#inverse-transform-sampling",
    "title": "Interesting things I learned this week (August 26th)",
    "section": "Inverse Transform Sampling",
    "text": "Inverse Transform Sampling\nYou put your random uniform number in, you get an exponential distribution out"
  },
  {
    "objectID": "posts/grfp/grfp.html",
    "href": "posts/grfp/grfp.html",
    "title": "GRFP Statements",
    "section": "",
    "text": "These are my GRFP material for 2015-2016."
  },
  {
    "objectID": "posts/grfp/grfp.html#research-proposal",
    "href": "posts/grfp/grfp.html#research-proposal",
    "title": "GRFP Statements",
    "section": "Research proposal",
    "text": "Research proposal\nDownload PDF file."
  },
  {
    "objectID": "posts/grfp/grfp.html#personal-statement",
    "href": "posts/grfp/grfp.html#personal-statement",
    "title": "GRFP Statements",
    "section": "Personal statement",
    "text": "Personal statement\nDownload PDF file."
  },
  {
    "objectID": "posts/agent_based_model/Untitled.html",
    "href": "posts/agent_based_model/Untitled.html",
    "title": "An agent based model of within-host malaria",
    "section": "",
    "text": "A little introduction to agent based model that I made out of boredom.\nDownload PDF file."
  },
  {
    "objectID": "posts/malthus/malthus.html",
    "href": "posts/malthus/malthus.html",
    "title": "It’s weird we don’t talk about Malthus",
    "section": "",
    "text": "Most Introduction to Ecology classes begin by introducing the exponential growth in a single-species population: \\(N(t) = N_0\\exp(rt).\\) In the required textbooks or readings, there is generally a reference or quote by Thomas Malthus that accompanies the equation. Most likely, the students will not give much passing thought about the accompanying quote. Honestly, when I first noticed it in my textbook, I just thought,\n“Huh some dead guy”\nA few months ago, I was listening to the musical called “Urinetown”. The premise is ridiculous as it involves people having to pay the local government to use the bathroom due to a severe water shortage (There’s a musical number called ’It’s a privilege to pee”). I won’t spoil the ending, but it’s a major downer ending. The last line of the musical is “Hail Malthus”. I remember thinking,\n“It’s the dead guy from my textbook appearing in a musical!”\nI then decided to do more research about Thomas Malthus. So after some research, I realized it’s weird we don’t talk about Malthus more."
  },
  {
    "objectID": "posts/malthus/malthus.html#a-case",
    "href": "posts/malthus/malthus.html#a-case",
    "title": "It’s weird we don’t talk about Malthus",
    "section": "",
    "text": "Most Introduction to Ecology classes begin by introducing the exponential growth in a single-species population. Generally, in the required textbooks or readings there is probably a reference or quote by Thomas Malthus that accompanies the equation. It is highly likely that the quote comes from his most famous treatise: An essay on the principle of population (1798).\nMost likely, the students will not give much passing thoughts about the accompanying quote. Honestly, when I first noticed it in my textbook, I just thought,\n“Huh some dead guy”\nA few months ago, I was listening to the musical called “Urinetown”. The premise is ridiculous and involves people having to pay the local government to use the bathroom due to a severe water shortage (There’s a musical number called ’It’s a privilege to pee”- it’s wild, highly recommend). I won’t spoil the ending, but it’s a major downer ending and the last line of the musical is “Hail Malthus”. Hey it’s the dead guy from my textbook appearing in a musical. I decided to do more research about this Thomas Malthus fellow.\nSo after some research, I realized it’s weird we don’t talk about Malthus more."
  },
  {
    "objectID": "posts/malthus/malthus.html#a-quick-background-to-thomas-malthus",
    "href": "posts/malthus/malthus.html#a-quick-background-to-thomas-malthus",
    "title": "It’s weird we don’t talk about Malthus",
    "section": "A quick background to Thomas Malthus",
    "text": "A quick background to Thomas Malthus\nThomas Malthus (born 1766) was an English economist and demographer. Malthus believed that people (especially the poor) just generally sucked. The idea of providing any welfare to the poor was preposterous as he believed that aid would make the poor unwilling to work. Malthus also believed that the poor would ultimately produce more children and continue the cycle of impoverishment. His huge dislike of the impoverished probably led to his most important work, “An essay on the principle of population”. The main thesis was that human population growth would ultimately outpace the food supply. As the population increases beyond what is sustainable, the poor would then ultimately suffer more. Therefore, the growth rate can only be countered by “positive checks” like famine and war.\nTo the untrained eye, that sounds like conventional wisdom (especially if you’re an asshole). If there was finite resources, unrestrained growth would lead to increased suffering as people would devour each other for food. However, this essay became the basis of horrific policies that led to immeasurable human suffering.\nSo it’s like… super weird to me we have this quote like in our ecology textbooks."
  },
  {
    "objectID": "posts/malthus/malthus.html#ireland-and-india",
    "href": "posts/malthus/malthus.html#ireland-and-india",
    "title": "It’s weird we don’t talk about Malthus",
    "section": "Ireland and India",
    "text": "Ireland and India\nWe know that the potato famine of Ireland was devastating and led to the mass exodus of its citizens. While the potato blight played a major role, we now know that that it was the British policies imposed on the Irish that mainly contributed to the mass starvation. It comes as a shock to modern audiences to learn that during the height of the famine, Ireland was exporting food at the time. While the people starved, grains were exported leading to massive food riots.\nSo where does Malthus fall in?\nHis student, Charles Trevalyn, was in charge of the Irish famine and he believed that no relief should be given to the people. In fact, he and many of the politicians in the British government believed the famine to be of great providence to teach the Irish the deadly sin of being… colonized and impoverished .\nReading academic literature on the economic policies of the Irish famine is honestly harrowing:\n\n“Those with the power to relieve famine convinced themselves that overly heroic exertions against implacable natural laws, whether of market prices or population growth, were worse than no effort at all” (Davis 2001: 32).\n\nBasically, Malthusian philosophy believed the famine to be a boon to the Irish people. Famine was the positive check on the growth rate of the people that the British found to be inferior.\nThis same thought appeared again in India, where again the economic policies of the British government were responsible for horrific suffering. I highly recommend the book ’Late Victorian Holocausts” by Mike Davis that show while famines are natural, economic policies of the ruling class greatly exacerbated the consequences. It’s harrowing to read how the colonists believed that this again was good for the poor. Railroads that were built through India were used to basically ship food out of the country. The poor were put in work camps that were inhumanely cruel.\n\n’I am profoundly persuaded that every rupee superfluously spent on famine relief only aggravates the evil effects of famine, and that in all such cases waste of money involves waste of life” - Lytton ( the viceroy at the time).\n\nThe Malthusian philosophy still haunts us to this day (Continued in a next article). I’m gonna just point out the stupid plan by Thanos in the Marvel movies."
  },
  {
    "objectID": "posts/malthus/malthus.html#so-we-talked-about-malthus",
    "href": "posts/malthus/malthus.html#so-we-talked-about-malthus",
    "title": "It’s weird we don’t talk about Malthus",
    "section": "So we talked about Malthus…",
    "text": "So we talked about Malthus…\nIt’s hard to remove the connection between exponential growth and Malthus. I feel that it is an important lesson to talk about how models can be used to enforce and justify human suffering. It is somewhat unnerving to know an equation that can was used to justify the starvation of millions.\nMaybe we should find a new quote to use? Though I think we shouldn’t try to hide Malthus and the legacy he left behind. Remember, it’s weird we don’t talk about Malthus."
  },
  {
    "objectID": "posts/finite/finite.html",
    "href": "posts/finite/finite.html",
    "title": "Finite versus instantaneous rate",
    "section": "",
    "text": "Okay this is embarrassing, but I was always confused about finite and instantaneous rate when I was a young grad student. So, let’s say you’re writing a model (let’s say of insects because it gets really depressing talking about people) and you gotta parameterize the daily mortality rate. You scour through some literature and find an experiment that says they found 10% of the individuals died in a day.\nFor simplicity sake, the daily mortality parameter is called \\(\\mu\\) and the insect population is \\(N(t)\\) and we’re going to use differential equations show below:\n\\[\\frac{dN(t)}{dt} = -\\mu N(t).\\]\nWithout thinking, let’s plug in 0.10 for \\(\\mu\\). But the problem is that this is a finite rate, we have to convert it first!"
  },
  {
    "objectID": "posts/waiting_time/waitingqmd.html",
    "href": "posts/waiting_time/waitingqmd.html",
    "title": "Exponential waiting time",
    "section": "",
    "text": "When you are diving in to a mathematical modeling paper with compartmental modeling, the authors may state that the waiting time within a compartment is exponentially distributed. This only comes up when the paper is pointing out how biologically unrealistic it is (another blog post). When I was an undergraduate, I was always puzzled by this! This concept of exponentially distributed dwelling-time is not intuitive without understanding some ordinary differential equation! Imagine a single compartment which we simply call A. The outflow out of the compartment is dependent on a constant rate \\(k\\) and the amount of individuals in A. So the easiest way to write this is:\n\\[\\frac{dA}{dt} = -kA. \\]\nThis equation means that the change in \\(A\\) over time is equal to \\(kA\\). This is a differential equation that we can actually solve analytically!\nThis is fairly simple to solve and let me show you the steps (honestly, I really recommend doing this yourself as practice!)\n\n\nImagine you have a stage A and there are individuals leaving it (note the hats and briefcases) and on the left, that is the solution of our differential equations. This is an exponential decay!\nSo now okay let’s look closer at the solution \\(A= A_0 exp(-kt)\\). First, we’re interested in how the proportion of individuals that stay in stage A change over time. To do this, we can divide the equation by A0 and get this:\n\\[\\frac{A}{A_0} = exp(-kt). \\]\nThis means that the proportion of individuals who are still in stage \\(A\\) can be represented by this exponential decay. Okay, what about the proportion leaving stage A? If 30 percent of the individuals are in stage A that means 70% left so the proportion leaving so \\(1 - \\frac{A}{A_0}\\) thus we get:\n\\[ 1- \\frac{A}{A0} = 1 -exp(-kt).\\]\nHm, that’s interesting! That looks exactly like the cumulative distribution function (CDF) of the exponential distribution! Remember that to get the probability distribution function (PDF), we just have find the derivative of the CDF:\n\\[\\frac{d}{dt} (1- \\frac{A}{A0}) = \\frac{d}{dt}(1 -exp(-kt)). \\]\nWhich means that the PDF is \\(k exp(-kt)\\)\nAnd if you remembered the mean of the exponential PDF is \\(\\frac{1}{k}\\) (We could have found the mean of the CDF, but I just thought it was easier).\nThat means that the mean time that individuals dwell in A is \\(\\frac{1}{k}\\).\nTherefore, for example if \\(k = \\frac{1}{2}\\) per day, then the mean time that an individual stay in \\(A\\) is \\(\\frac{1}{(1/2)}\\) is 2 days.\nHopefully, I hope this makes it clear using differential equations that there is an exponential distribution. The exponentially distributed waiting time is an assumption! It can be biologically unrealistic because it assumes that that most individuals leave the compartment almost instantly."
  },
  {
    "objectID": "posts/stochastic_sis/stochastic_sis.html#the-math",
    "href": "posts/stochastic_sis/stochastic_sis.html#the-math",
    "title": "Stochastic SIS model",
    "section": "The math",
    "text": "The math\nAssume we have an SIS model (Susceptible to Infected to Susceptible). Assuming we have constant \\(N\\) individuals in the population (no demography), there can be \\(n\\) number of infectious individuals. Therefore, for the susceptible population, \\(S = N- n\\) and for the infectious population \\(I = n\\) .\nIn a stochastic, markovian model. We can have individuals of both state transition like so:\n\\[\n(S,I) \\rightarrow (S-1, I + 1) ,\n\\]\nwhere a susceptible individual becomes infected with the parameter \\(\\beta\\) or:\n\\[\n(S,I) \\rightarrow (S+1, I -1),\n\\]\nwhere infected individuals recover at rate \\(\\gamma\\).\nFor stochastic modeling, we use something called a master equation that can tell us how the state distribution varies over time (how individuals “jump” from one state to another).\nIf we’re interested in\nAssuming, we are interested in looking the probability where \\(n\\) individuals are infected and we are interested what the next time step could be. We call this \\(p_n (t)\\).\nWell, it could be that in a state where \\(n-1\\) individuals are infected, the infected individual infected one of the susceptible. This means that\n\\[\np_{n-1} \\beta(n-1)(N-(n-1)) \\Delta t\n\\]\nin English, the probability of being in the state where there are \\(n-1\\) infectious individuals that will then infect another individual which is the number of susceptibles (\\(N-(n-1)\\)) and infectious individuals (\\(n-1\\)).\nNow it could be possible that there is the probability that susceptible individuals do not get infected and the infectious individuals have not recovered yet.\nThis means that the probability of no individual being infected is $ 1- P(an individual being infected)$, such that:\n$$ p_n(1- (n(N-n)))\n$$ And individuals that do not recover is:\n\\[\np_n(1-(\\beta n(N-n)+\\gamma n)) \\Delta t\n\\] Then we can have an individual recover:\n\\[\np_{n+1} = \\gamma (n+1) \\Delta t\n\\] Putting this all together:\n\\[\np_n(t + \\Delta t) = p_{n-1} \\beta(n-1)(N-(n-1)) \\Delta t+\np_n(1-(\\beta n(N-n)-\\gamma n)) \\Delta t +\n\\gamma (n+1) \\Delta t\n\\] We notice that we can convert this into a continous form by noting the \\(p_n\\) in the middle term.\n\\[\np_n(t + \\Delta t) - p_n(t)= p_{n-1} \\beta(n-1)(N-(n-1)) \\Delta t\n-p_n(\\beta n(N-n)+\\gamma n)) \\Delta t +\n\\gamma (n+1) \\Delta t\n\\] We can then divide everything by the time step \\(\\Delta t\\):\n\\[\n\\frac{p_n(t + \\Delta t) - p_n(t)}{\\Delta t}= p_{n-1} \\beta(n-1)(N-(n-1))\n-p_n(\\beta n(N-n)+\\gamma n)) +\n\\gamma (n+1)\n\\] And as \\(\\Delta t\\) approaches infinity, we then have\n\\[\n\\frac{dP_n(t)}{dt} =  p_{n-1} \\beta(n-1)(N-(n-1))\n-p_n(\\beta n(N-n)+\\gamma n)) +\n\\gamma (n+1)\n\\] With this master equation, we can simulate the model."
  },
  {
    "objectID": "posts/stochastic_sis/stochastic_sis.html#code",
    "href": "posts/stochastic_sis/stochastic_sis.html#code",
    "title": "Stochastic SIS model",
    "section": "Code",
    "text": "Code\nThe Gillepsie method takes that master equation. Where we first simulate the time to the next event and figure out which event has occcured (infection or recovery). We use something called a Poisson process where we say there is a rate at which events happen but we want it to be random AND independent. The rate at which something happens in the system is what we described with the master equation. The poisson process is also useful because we also know the time to events.\nPoisson (P_n) then the exponential waiting time is (1- exp(P_n)t).\nWe can use a uniform distribution through something called:Inverse Transform Sampling.\n\n# We create a susceptible and infectious vector and we will simulate for a 100 time steps\nS = matrix(c(10,rep(0,100-1)), nrow = 100, ncol = 1 ) #susceptible individuals \nI = matrix(c(2,rep(0,100-1)), nrow = 100,ncol = 1) #infectious individuals\n\nHere are the parameters:\n\nbeta = 0.1\ngamma = 1/3\ntime = 0 \n\n\nfor (i in seq(1,100)){\n  \n  infection = beta * S[i] * I[i]\n  recovery = gamma * I[i]\n  total_rate = infection + recovery\n  \n  random_number&lt;- runif(2,min=0,max=1)\n  \n  \n  tau &lt;- (1 /total_rate) * log(1 / random_number[1])\n  \n  time = time +  tau\n  \n  rate_of_interest &lt;- infection/total_rate\n  \n  if(random_number[2] &lt; rate_of_interest){\n    \n    S[i+1] = S[i] - 1 \n    I[i+1] = I[i] + 1\n  }\n  else{\n     S[i+1] = S[i] + 1 \n     I[i+1] = I[i] - 1  \n    \n  }\n}\n\n\nfull_model&lt;- cbind.data.frame(time = seq(0,100),Sus = S, Infect = I)\n\n\nggplot(full_model, aes(x = time, y = S))+ geom_line(color = 'red') + \n  geom_line(aes(x = time, y= I),color = 'black')+theme_bw()"
  },
  {
    "objectID": "posts/stochastic_sis/stochastic_sis.html#doing-this-in-a-network",
    "href": "posts/stochastic_sis/stochastic_sis.html#doing-this-in-a-network",
    "title": "Stochastic SIS model",
    "section": "Doing this in a network",
    "text": "Doing this in a network\n\n#let's say there are 10 individuals \n\ngraph = erdos.renyi.game(25,50 , type = \"gnm\", directed = FALSE)\nV(graph)$label &lt;- seq(1,25)\nstate = rep(\"S\",25)\npatient_zero = sample(seq(1,25),1)\nstate[patient_zero] &lt;- \"I\"\n\n time_list = NULL\nfor (i in seq(1,100)){\n\n  infection_propensity = NULL\n  recovery_propensity = NULL\n\n  inf_node &lt;- which(state == \"I\")\n    \n  \n    recovery_propensity &lt;- list(cbind(n=  inf_node,rate=gamma))\n    neighbors &lt;-  neighbors(graph, inf_node )\n     \n     for (n in neighbors){\n        if (state[n] == \"S\") {\n         infection_propensity[[n]] &lt;- cbind(n,rate=beta)\n       \n        }\n       \n     }\n    \n    \n    full_infect_rate &lt;- cbind.data.frame(do.call(rbind,infection_propensity),status = 'infect')\n    full_recovery_rate &lt;- cbind.data.frame(do.call(rbind,recovery_propensity), status = 'recover')\n\n    events = rbind(full_infect_rate, full_recovery_rate)\n    \n     \n    total_rate &lt;- sum(events$rate)\n    \n     random &lt;- runif(2)\n    \n      tau &lt;- -log(random[1]) / total_rate\n     \n      cumulative_rate &lt;- 0\n      \n      selected_event &lt;- NULL\n      for (event in seq(1,nrow(events))) {\n      cumulative_rate &lt;- cumulative_rate + events$rate[event]\n      \n      if (random[2] * total_rate &lt;= cumulative_rate) {\n        selected_event &lt;- event\n        break\n      }\n    }\n       time &lt;- time + tau\n      \n      if (!is.null(selected_event)) {\n      node &lt;- selected_event\n      if (state[node] == \"I\") {\n        # Recovery event\n        state[node] &lt;- \"S\"\n      } else {\n        # Infection event\n        state[node] &lt;- \"I\"\n      }\n    }\n \n       time_list[[i]] = cbind.data.frame(time,state,node = seq(1,100))\n}      \n\n full_time_list &lt;- do.call(rbind, \n time_list )  \n\n\nfull_graph&lt;- fortify(graph)\nfull_spatial &lt;-full_graph[,c('x','y','label')]\nfull_spatial&lt;- full_spatial[!(duplicated(full_spatial)), ]\n\neep&lt;- left_join( full_time_list ,full_spatial, by = join_by(node==label))\n\ngg_anim &lt;- ggplot(data=full_graph)+\n  geom_edges(aes(x = x, y = y, xend = xend, yend = yend),color = \"black\")+\n  geom_point(data = eep, aes(x =x, y=y,color = state),size =10)+\n   transition_states(time,  state_length = 1)+\n  enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out',interval = 0.001)+theme_void()\n\n  animate(gg_anim, fps=5)\n\nWarning: Removed 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 75 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n anim_save ('gganim.gif')"
  },
  {
    "objectID": "posts/interesting_stuff_I_learned_part2/interestingstuffilearned2.html#stochastic-simulation-is-hard",
    "href": "posts/interesting_stuff_I_learned_part2/interestingstuffilearned2.html#stochastic-simulation-is-hard",
    "title": "Interesting things I learned this week (August 26th)",
    "section": "Stochastic simulation is hard",
    "text": "Stochastic simulation is hard\nBut this guy has this blog: https://lewiscoleblog.com/gillespie-algorithm"
  },
  {
    "objectID": "posts/interesting_paper_week3/interesting_paperweek3.html",
    "href": "posts/interesting_paper_week3/interesting_paperweek3.html",
    "title": "Interesting stuff I read (September 3rd)",
    "section": "",
    "text": "Elliot et al. 2003\nAn old paper but I really enjoy the writing style. I know some people don’t like it but I love papers that have series of rhetorical questions. Simple model about how parasite virulence can decrease with the more mobile of the hosts (which is generally the vector). Though the authors point out that this is only true with two assumptions: (1) interpatch movement of free parasites are weak and (2) competitive displacement among patches are weak. I guess the first assumption is that vectors matter less if free parasites are able to disperse (keeping the vectors alive become less important)."
  },
  {
    "objectID": "posts/interesting_paper_week3/interesting_paperweek3.html#how-virulent-should-a-parasite-be-to-its-vector",
    "href": "posts/interesting_paper_week3/interesting_paperweek3.html#how-virulent-should-a-parasite-be-to-its-vector",
    "title": "Interesting stuff I read (September 3rd)",
    "section": "",
    "text": "Elliot et al. 2003\nAn old paper but I really enjoy the writing style. I know some people don’t like it but I love papers that have series of rhetorical questions. Simple model about how parasite virulence can decrease with the more mobile of the hosts (which is generally the vector). Though the authors point out that this is only true with two assumptions: (1) interpatch movement of free parasites are weak and (2) competitive displacement among patches are weak. I guess the first assumption is that vectors matter less if free parasites are able to disperse (keeping the vectors alive become less important)."
  },
  {
    "objectID": "posts/spatial_network/spatialnetwork.html",
    "href": "posts/spatial_network/spatialnetwork.html",
    "title": "Simulating spatial network (Part 2)",
    "section": "",
    "text": "This is a step by step guide for how we simulate the spatial network for tritonet."
  },
  {
    "objectID": "posts/spatial_network/spatialnetwork.html#sample-the-x-and-y-coordinates-for-the-patches.",
    "href": "posts/spatial_network/spatialnetwork.html#sample-the-x-and-y-coordinates-for-the-patches.",
    "title": "Simulating spatial network (Part 2)",
    "section": "1. Sample the x and y coordinates for the patches.",
    "text": "1. Sample the x and y coordinates for the patches.\n\nset.seed(24601) # Set the seed number\nmax_distance &lt;- 20 # Set the maximum limit of the xy plane.\n\nWe use the sample function to randomly select both the x (longitude) and y (latitude) coordinates for each node. Using dist, we can then calculate the distance matrix for the pairwise distances between all nodes. The weight of the edges is then calculated using a negative exponential kernel.\n\nxy &lt;- seq(1, max_distance, length.out = 2000) ### All possible coordinates\nx_coord &lt;- sample(xy, 100, replace = TRUE) # x-coordinate\ny_coord &lt;- sample(xy, 100, replace = TRUE) # y-coordinate\nxy_coord &lt;- cbind(x_coord, y_coord) # xy-coordinates combined\nNegExpDist &lt;- as.matrix(exp(-dist(xy_coord))) # distance matrix with neg. exp kernel"
  },
  {
    "objectID": "posts/spatial_network/spatialnetwork.html#convert-the-distance-matrix-into-an-adjacency-matrix",
    "href": "posts/spatial_network/spatialnetwork.html#convert-the-distance-matrix-into-an-adjacency-matrix",
    "title": "Simulating spatial network (Part 2)",
    "section": "2. Convert the distance matrix into an adjacency matrix",
    "text": "2. Convert the distance matrix into an adjacency matrix\n\nAdj_graph &lt;- graph_from_adjacency_matrix(NegExpDist,\n  mode = \"undirected\",\n  diag = FALSE,\n  weighted = TRUE\n)\n\n## Adding latitude and longitude\nV(Adj_graph)$Long &lt;- xy_coord[, 1] # x-coordinates\nV(Adj_graph)$Lat &lt;- xy_coord[, 2] # y-coordinates\n\nWhen we plot the network, we can see that all nodes are connected to each other."
  },
  {
    "objectID": "posts/spatial_network/spatialnetwork.html#reduce-the-edges",
    "href": "posts/spatial_network/spatialnetwork.html#reduce-the-edges",
    "title": "Simulating spatial network (Part 2)",
    "section": "3. Reduce the edges",
    "text": "3. Reduce the edges\nWe are going to delete the majority of the edges. We assume a very low connectance (1%) and we back-calculate the number of edges that we must keep.\n\nnumber_of_edges &lt;- (0.01 * (100^2))\n\nWe choose the top 100 highest-weight edges and delete all other edges.\n\n### If the number of edges required for connectance is 100, then\n### choose the 100 likeliest (highest weight) edges.\ndeleted_edges_graph &lt;- delete_edges(\n  Adj_graph,\n  which(E(Adj_graph)$weight &lt; sort(E(Adj_graph)$weight,\n    decreasing = T\n  )[number_of_edges])\n)\n\nThis is what it looks like now; we can see that there are components"
  },
  {
    "objectID": "posts/spatial_network/spatialnetwork.html#choose-the-component-with-the-greatest-number-of-nodes",
    "href": "posts/spatial_network/spatialnetwork.html#choose-the-component-with-the-greatest-number-of-nodes",
    "title": "Simulating spatial network (Part 2)",
    "section": "4. Choose the component with the greatest number of nodes",
    "text": "4. Choose the component with the greatest number of nodes\nUsing the function decompose, we can split the network into smaller networks.\n\ndecomposed_components &lt;- decompose(deleted_edges_graph)\n\n# Count the number of nodes for each component and then give me\n### the index for the largest component.\n\nbiggest_component_length &lt;- which.max(lapply(\n  decomposed_components,\n  function(x) {\n    vcount(x)\n  }\n))\n\n### retrieve our network of interest\nnetwork_of_interest &lt;- decomposed_components[[biggest_component_length]]\n\nThis is the network of interest (the biggest component)"
  },
  {
    "objectID": "posts/spatial_network/spatialnetwork.html#calculate-the-new-connectance",
    "href": "posts/spatial_network/spatialnetwork.html#calculate-the-new-connectance",
    "title": "Simulating spatial network (Part 2)",
    "section": "5. Calculate the new connectance",
    "text": "5. Calculate the new connectance\nI create a function called connectance_calculator that calculates the connectance when given the number of nodes and edges.\n\n### calculate the connectance by inputting the number of nodes and the number of\n### edges\n\nconnectance_calculator &lt;- function(nodes, edges) {\n  return(edges / (nodes^2))\n}\n\n\nconnectance_calculator(\n  vcount(network_of_interest),\n  ecount(network_of_interest)\n)\n\n[1] 0.214876"
  },
  {
    "objectID": "posts/spatial_network/spatialnetwork.html#create-an-empty-list-to-populate-with-igraph-objects",
    "href": "posts/spatial_network/spatialnetwork.html#create-an-empty-list-to-populate-with-igraph-objects",
    "title": "Simulating spatial network (Part 2)",
    "section": "6. Create an empty list to populate with igraph objects",
    "text": "6. Create an empty list to populate with igraph objects\nLet’s create a list:\n\nadj_list &lt;- NULL # For the actual igraph\nadj_info_list &lt;- NULL # For information about each igraph\n\nLet’s manually add the first one in.\n\nadj_list[[1]] &lt;- network_of_interest #put the igraph object in.\n\n\nadj_info_list[[1]] &lt;- c( \n  num_nodes = vcount(network_of_interest),\n  num_edges = ecount(network_of_interest),\n  connectance = connectance_calculator(\n    vcount(network_of_interest),\n    ecount(network_of_interest)\n  )\n)"
  },
  {
    "objectID": "posts/spatial_network/spatialnetwork.html#add-edges-one-by-one",
    "href": "posts/spatial_network/spatialnetwork.html#add-edges-one-by-one",
    "title": "Simulating spatial network (Part 2)",
    "section": "7. Add edges one by one",
    "text": "7. Add edges one by one\nFirst, we calculate the new distance matrix of the component network of interest:\n\n# Get the x-y coordinates\nxy_coord_interest &lt;- cbind(\n  V(network_of_interest)$Long,\n  V(network_of_interest)$Lat\n)\n\n# Calculate new distance matrix\nDispMat_interest &lt;- as.matrix(exp(-dist(xy_coord_interest)))\n\nWe get the edge list of the network of interest.\n\nedgelist_of_interest &lt;- as_edgelist(network_of_interest, names = F)\n\n### The columns show the patches that are connected by an edge\nhead(edgelist_of_interest)\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    1    4\n[3,]    4    5\n[4,]    1    6\n[5,]    4    6\n[6,]    5    6\n\n\nBy melting the distance matrix, we can then get a data.frame that shows the edge connections between the different nodes as well as the edge weights.\n\nmelted_edge_list &lt;- melt(DispMat_interest)\n\n### patch1, patch2, and weight are the new column names\ncolnames(melted_edge_list) &lt;- c(\"patch1\", \"patch2\", \"weight\")\n\nWe want to remove rows from melted_edge_list that already exist in the network_of_interest. This is because we’re interested in adding new edges that do not currently exist in the network.\n\nnew_distance &lt;- subset(\n  melted_edge_list,\n  !(paste0(\n    melted_edge_list$patch1, \"-\",\n    melted_edge_list$patch2\n  )\n  %in%\n    paste0(\n      edgelist_of_interest[, 1],\n      \"-\", edgelist_of_interest[, 2]\n    )\n  )\n)\n\nLet us order the new data.frame by the edge weight:\n\nnew_distance &lt;- new_distance[order(new_distance$weight, decreasing = TRUE), ]\nhead(new_distance)\n\n   patch1 patch2    weight\n44     11      4 0.7933777\n76     10      7 0.7360045\n39      6      4 0.6963310\n7       7      1 0.5880216\n66     11      6 0.5739138\n20      9      2 0.5385340\n\n\nWe’re going to loop this, but just to how what is happening. We add an edge between patch1 and patch2 as well as its weight.\n\nnetwork_of_interest_added &lt;- network_of_interest + edge(\n  c(\n    new_distance[1, \"patch1\"],\n    new_distance[1, \"patch2\"]\n  ),\n  weight = new_distance[1, \"weight\"]\n)\n\nAgain, we’re going to automate this, but we are going to add the information we need to the lists that we made earlier.\n\nadj_list[[2]] &lt;- network_of_interest_added\nadj_info_list[[2]] &lt;- c(\n  num_nodes = vcount(network_of_interest_added),\n  num_edges = ecount(network_of_interest_added),\n  connectance = connectance_calculator(\n    vcount(network_of_interest_added),\n    ecount(network_of_interest_added)\n  )\n)"
  },
  {
    "objectID": "posts/spatial_network/spatialnetwork.html#loop-through.",
    "href": "posts/spatial_network/spatialnetwork.html#loop-through.",
    "title": "Simulating spatial network (Part 2)",
    "section": "8. Loop through.",
    "text": "8. Loop through.\n\nfor (i in seq(2, nrow(new_distance))) {\n  network_of_interest_added &lt;- network_of_interest_added + edge(\n    c(\n      new_distance[i, \"patch1\"],\n      new_distance[i, \"patch2\"]\n    ),\n    weight = new_distance[i, \"weight\"]\n  )\n\n\n  adj_list[[i + 1]] &lt;- network_of_interest_added\n  adj_info_list[[i + 1]] &lt;- c(\n    num_nodes = vcount(network_of_interest_added),\n    num_edges = ecount(network_of_interest_added),\n    connectance = connectance_calculator(\n      vcount(network_of_interest_added),\n      ecount(network_of_interest_added)\n    )\n  )\n}"
  },
  {
    "objectID": "posts/spatial_network/spatialnetwork.html#check-that-there-is-a-positive-relationship-with-edge-number-and-connectance",
    "href": "posts/spatial_network/spatialnetwork.html#check-that-there-is-a-positive-relationship-with-edge-number-and-connectance",
    "title": "Simulating spatial network (Part 2)",
    "section": "9. Check that there is a positive relationship with edge number and connectance",
    "text": "9. Check that there is a positive relationship with edge number and connectance\n\nadj_info_df &lt;- data.frame(do.call(rbind, adj_info_list))\n\n\nggplot(adj_info_df, aes(x = num_edges, y = connectance)) +\n  geom_point() +\n  ylab(\"Connectance\") +\n  xlab(\"Number of edges\") +\n  theme_classic() +\n  theme(\n    axis.text = element_text(size = 14),\n    axis.title = element_text(size = 15)\n  )\n\n\n\n\n\n\n\n\nWe can see that by increasing the number of edges, we also increase the connectance."
  },
  {
    "objectID": "posts/spatial_network/spatialnetwork.html#creating-the-full-function",
    "href": "posts/spatial_network/spatialnetwork.html#creating-the-full-function",
    "title": "Simulating spatial network (Part 2)",
    "section": "10. Creating the full function",
    "text": "10. Creating the full function\nGoing to be a huge function so break it into much smaller sub-functions.\nThe function simulate_xy_coordinates corresponds to Step 1 (Sample x-y coordinates for the patches). The output should be a list with the first element being the data.frame holding the x and y coordinates of the nodes and the second element being the distance matrix.\n\nsimulate_xy_coordinates &lt;- function(seed = 24601, max_distance) {\n  set.seed(seed)\n  xy &lt;- seq(1, max_distance, length.out = 2000) ### List of all possible coordinates\n  x_coord &lt;- sample(xy, 100, replace = TRUE) # x-coordinate\n  y_coord &lt;- sample(xy, 100, replace = TRUE) # y-coordinate\n  xy_coord &lt;- cbind(x_coord, y_coord) # xy-coordinates combined\n  NegExpDist &lt;- as.matrix(exp(-dist(xy_coord))) # distance matrice with kernel\n\n  return(list(xy_coord, NegExpDist))\n}\n\nThe function retrieve_biggest_component corresponds to Step 2 (Convert the distance matrix into an adjacency matrix), Step 3 (Reduce the edges), and Step 4 (Choose the components with the greatest number of nodes). The input takes the list element from simulate_xy_coordinate and returns the network of interest.\n\nretrieve_biggest_component &lt;- function(list) {\n  Adj_graph &lt;- graph_from_adjacency_matrix(list[[2]],\n    mode = \"undirected\",\n    diag = FALSE,\n    weighted = TRUE\n  )\n\n  ## Adding latitude and longitude\n  V(Adj_graph)$Long &lt;- list[[1]][, 1] # x-coordinates\n  V(Adj_graph)$Lat &lt;- list[[1]][, 2] # y-coordinates\n\n  number_of_edges &lt;- (0.01 * (100^2))\n\n  deleted_edges_graph &lt;- delete_edges(\n    Adj_graph,\n    which(E(Adj_graph)$weight &lt; sort(E(Adj_graph)$weight,\n      decreasing = T\n    )[number_of_edges])\n  )\n\n\n  decomposed_components &lt;- decompose(deleted_edges_graph)\n\n  # Count the number of nodes for each componenent and then give me\n  ### the index for the largest.\n  biggest_component_length &lt;- which.max(lapply(\n    decomposed_components,\n    function(x) {\n      vcount(x)\n    }\n  ))\n\n  ### retrieve our network of interest\n  network_of_interest &lt;- decomposed_components[[biggest_component_length]]\n\n  return(network_of_interest)\n}\n\nThe function recalculate_distance_matrix correspond to the first half of step 5 (Add edges one by one). You input the network of interest and should return a data.frame that has all the possible edges (that are not in the current network) sorted in decreasing order of edge weight.\n\nrecalculate_distance_matrix &lt;- function(network) {\n  # Get the x-y coordinates\n  xy_coord_interest &lt;- cbind(\n    V(network)$Long,\n    V(network)$Lat\n  )\n\n  # Calculate new distance matrices\n  DispMat_interest &lt;- as.matrix(exp(-dist(xy_coord_interest)))\n\n  edgelist_of_interest &lt;- as_edgelist(network, names = F)\n\n  melted_edge_list &lt;- melt(DispMat_interest)\n\n  colnames(melted_edge_list) &lt;- c(\"patch1\", \"patch2\", \"weight\")\n\n\n  new_distance &lt;- subset(\n    melted_edge_list,\n    !(paste0(\n      melted_edge_list$patch1, \"-\",\n      melted_edge_list$patch2\n    )\n    %in%\n      paste0(\n        edgelist_of_interest[, 1],\n        \"-\", edgelist_of_interest[, 2]\n      )\n    )\n  )\n\n  new_distance_df &lt;- new_distance[order(new_distance$weight, decreasing = TRUE), ]\n\n  return(new_distance_df)\n}\n\nThe full function thus looks like this:\n\nsimulate_spatial_network &lt;- function(seed, max_distance) {\n        \n  list_xy_coord &lt;- simulate_xy_coordinates(seed, max_distance)\n  network_interest &lt;- retrieve_biggest_component(list_xy_coord)\n  possible_edges_df &lt;- recalculate_distance_matrix(network_interest)\n  adj_list &lt;- NULL\n  adj_info_list &lt;- NULL\n\n  ### Manually add the first network in\n\n  adj_list[[1]] &lt;- network_interest\n  adj_info_list[[1]] &lt;- c(\n    num_nodes = vcount(network_interest),\n    num_edges = ecount(network_interest),\n    connectance = connectance_calculator(\n      vcount(network_interest),\n      ecount(network_interest)\n    )\n  )\n\n  ### For loop time\n  for (new_edge in seq(1, nrow(possible_edges_df))) {\n    network_interest &lt;- network_interest + edge(c(new_distance[new_edge, \"patch1\"], new_distance[new_edge, \"patch2\"]),\n      weight = new_distance[new_edge, \"weight\"]\n    )\n\n    adj_list[[new_edge + 1]] &lt;- network_interest\n    adj_info_list[[new_edge + 1]] &lt;- c(\n      num_nodes = vcount(network_interest),\n      num_edges = ecount(network_interest),\n      connectance = connectance_calculator(\n        vcount(network_interest),\n        ecount(network_interest)\n      )\n    )\n  }\n  return(list(adj_list, do.call(rbind, adj_info_list)))\n}"
  },
  {
    "objectID": "posts/spatial_network/spatialnetwork.html#testing-the-full-function",
    "href": "posts/spatial_network/spatialnetwork.html#testing-the-full-function",
    "title": "Simulating spatial network (Part 2)",
    "section": "11. Testing the full function",
    "text": "11. Testing the full function\n\nsimulated_list &lt;- simulate_spatial_network (24601, 20)\n\n\nprint(simulated_list[[2]])\n\n      num_nodes num_edges connectance\n [1,]        11        26   0.2148760\n [2,]        11        27   0.2231405\n [3,]        11        28   0.2314050\n [4,]        11        29   0.2396694\n [5,]        11        30   0.2479339\n [6,]        11        31   0.2561983\n [7,]        11        32   0.2644628\n [8,]        11        33   0.2727273\n [9,]        11        34   0.2809917\n[10,]        11        35   0.2892562\n[11,]        11        36   0.2975207\n[12,]        11        37   0.3057851\n[13,]        11        38   0.3140496\n[14,]        11        39   0.3223140\n[15,]        11        40   0.3305785\n[16,]        11        41   0.3388430\n[17,]        11        42   0.3471074\n[18,]        11        43   0.3553719\n[19,]        11        44   0.3636364\n[20,]        11        45   0.3719008\n[21,]        11        46   0.3801653\n[22,]        11        47   0.3884298\n[23,]        11        48   0.3966942\n[24,]        11        49   0.4049587\n[25,]        11        50   0.4132231\n[26,]        11        51   0.4214876\n[27,]        11        52   0.4297521\n[28,]        11        53   0.4380165\n[29,]        11        54   0.4462810\n[30,]        11        55   0.4545455\n[31,]        11        56   0.4628099\n[32,]        11        57   0.4710744\n[33,]        11        58   0.4793388\n[34,]        11        59   0.4876033\n[35,]        11        60   0.4958678\n[36,]        11        61   0.5041322\n[37,]        11        62   0.5123967\n[38,]        11        63   0.5206612\n[39,]        11        64   0.5289256\n[40,]        11        65   0.5371901\n[41,]        11        66   0.5454545\n[42,]        11        67   0.5537190\n[43,]        11        68   0.5619835\n[44,]        11        69   0.5702479\n[45,]        11        70   0.5785124\n[46,]        11        71   0.5867769\n[47,]        11        72   0.5950413\n[48,]        11        73   0.6033058\n[49,]        11        74   0.6115702\n[50,]        11        75   0.6198347\n[51,]        11        76   0.6280992\n[52,]        11        77   0.6363636\n[53,]        11        78   0.6446281\n[54,]        11        79   0.6528926\n[55,]        11        80   0.6611570\n[56,]        11        81   0.6694215\n[57,]        11        82   0.6776860\n[58,]        11        83   0.6859504\n[59,]        11        84   0.6942149\n[60,]        11        85   0.7024793\n[61,]        11        86   0.7107438\n[62,]        11        87   0.7190083\n[63,]        11        88   0.7272727\n[64,]        11        89   0.7355372\n[65,]        11        90   0.7438017\n[66,]        11        91   0.7520661\n[67,]        11        92   0.7603306\n[68,]        11        93   0.7685950\n[69,]        11        94   0.7768595\n[70,]        11        95   0.7851240\n[71,]        11        96   0.7933884\n[72,]        11        97   0.8016529\n[73,]        11        98   0.8099174\n[74,]        11        99   0.8181818\n[75,]        11       100   0.8264463\n[76,]        11       101   0.8347107\n[77,]        11       102   0.8429752\n[78,]        11       103   0.8512397\n[79,]        11       104   0.8595041\n[80,]        11       105   0.8677686\n[81,]        11       106   0.8760331\n[82,]        11       107   0.8842975\n[83,]        11       108   0.8925620\n[84,]        11       109   0.9008264\n[85,]        11       110   0.9090909\n[86,]        11       111   0.9173554\n[87,]        11       112   0.9256198\n[88,]        11       113   0.9338843\n[89,]        11       114   0.9421488\n[90,]        11       115   0.9504132\n[91,]        11       116   0.9586777\n[92,]        11       117   0.9669421\n[93,]        11       118   0.9752066\n[94,]        11       119   0.9834711\n[95,]        11       120   0.9917355\n[96,]        11       121   1.0000000"
  },
  {
    "objectID": "posts/agent_based_model/malaria.html",
    "href": "posts/agent_based_model/malaria.html",
    "title": "An agent based model of within-host malaria",
    "section": "",
    "text": "This is a simple introduction to an agent based model that I made out of boredom. So imagine you’re a malaria parasite inside the bloodstream of your host. You must invade a red blood cell or you must perish! I think this shows how much daughter parasites you make is dependent on how long you can live.\n\n\n\n\n \n\n \n\n  \n    \n       \n       \n    \n       \n    \n    \nCopyright 2024, Damie Pak"
  },
  {
    "objectID": "posts/abouheif_mean_C/abouheifqmd.html",
    "href": "posts/abouheif_mean_C/abouheifqmd.html",
    "title": "Abouheif Mean C",
    "section": "",
    "text": "A very silly thing I made to understand Abouheif Mean C (a non-complicated way to see if there is a phylogenetic signal). Sometimes to fully learn something, I need to do it tediously."
  },
  {
    "objectID": "posts/malthus/malthus.html#my-case",
    "href": "posts/malthus/malthus.html#my-case",
    "title": "It’s weird we don’t talk about Malthus",
    "section": "",
    "text": "Most Introduction to Ecology classes begin by introducing the exponential growth in a single-species population: \\(N(t) = N_0\\exp(rt).\\) In the required textbooks or readings, there is generally a reference or quote by Thomas Malthus that accompanies the equation. Most likely, the students will not give much passing thought about the accompanying quote. Honestly, when I first noticed it in my textbook, I just thought,\n“Huh some dead guy”\nA few months ago, I was listening to the musical called “Urinetown”. The premise is ridiculous as it involves people having to pay the local government to use the bathroom due to a severe water shortage (There’s a musical number called ’It’s a privilege to pee”). I won’t spoil the ending, but it’s a major downer ending. The last line of the musical is “Hail Malthus”. I remember thinking,\n“It’s the dead guy from my textbook appearing in a musical!”\nI then decided to do more research about Thomas Malthus. So after some research, I realized it’s weird we don’t talk about Malthus more."
  },
  {
    "objectID": "posts/finite/finite.html#introduction",
    "href": "posts/finite/finite.html#introduction",
    "title": "Finite versus instantaneous rate",
    "section": "",
    "text": "Okay this is embarrassing, but I was always confused about finite and instantaneous rate when I was a young grad student. So, let’s say you’re writing a model (let’s say of insects because it gets really depressing talking about people) and you gotta parameterize the daily mortality rate. You scour through some literature and find an experiment that says they found 10% of the individuals died in a day.\nFor simplicity sake, the daily mortality parameter is called \\(\\mu\\) and the insect population is \\(N(t)\\) and we’re going to use differential equations show below:\n\\[\\frac{dN(t)}{dt} = -\\mu N(t).\\]\nWithout thinking, let’s plug in 0.10 for \\(\\mu\\). But the problem is that this is a finite rate, we have to convert it first!"
  },
  {
    "objectID": "posts/finite/finite.html#continuous-versus-discrete-death",
    "href": "posts/finite/finite.html#continuous-versus-discrete-death",
    "title": "Finite versus instantaneous rate",
    "section": "Continuous versus discrete death",
    "text": "Continuous versus discrete death\nLet’s think about it, when we talk about differential equations we have to realize that they’re continuous! Meaning that while we like to think about like modeling population dynamics in the manner of days (i.e 10 insects die every day), we have to realize that there are time units smaller than days: hours, minutes, seconds. And at these tiny time steps, there is still death! As differential equations are continuous, that means that death is a continuous process.\nOkay, you found that the experiment found that 10% of the insects die in a day. But that is a finite rate! You’re not assuming insects are dying continuously throughout that one day. In fact, it’s like they’re waiting to die at the end of each day!"
  },
  {
    "objectID": "posts/finite/finite.html#how-to-convert",
    "href": "posts/finite/finite.html#how-to-convert",
    "title": "Finite versus instantaneous rate",
    "section": "How to convert",
    "text": "How to convert\nSo what does that mean?  It means you have to take your finite rate and convert it to an instantaneous rate. How do you do that?\nWell let’s look at the differential equation here again:\n\\[ \\frac{dN}{dt} = -\\mu N(t)\\]\nIf we solve it then it becomes\n\\[N(t) = N_0 * \\exp(-\\mu t).\\]\nOkay so let’s say we have an initial conditions of 100, so \\(N_0 = 100\\). I want to solve for \\(\\mu\\). So let’s divide both parts of the equation with \\(N_0\\) and that means it would \\(\\frac{N(t)}{N_0}\\). Okay so if we see that 10% have died in one day that means \\(N(1) = 90\\) \\((100- (100 * 0.10) = 90)\\). Substituting the new numbers in and setting \\(t = 1\\) since we’re looking at one day only,we get:\n\\[\\frac{90}{100} = \\exp(-\\mu t) = \\exp(-\\mu).\\]\nWe can take the natural log to get: \\(log(\\frac{90}{100}) = -\\mu\\). When we calculate it we get \\(\\mu = 0.105\\). Aha, close to 0.10 but not exactly 0.10? What was the entire point of this exercise? Well, the finite and instantaneous rates actually diverge as the finite mortality rate increases. Say that 70% of individuals die in a day. So okay, rerun the above. \\(\\mu = -ln(\\frac{30}{100}) = 1.20.\\) That’s a lot bigger than 0.70! Remember that RATES allow for a value greater than 1.\nIf you want more information check:\nhttps://influentialpoints.com/Training/finite-and-instantaneous_rates.htm"
  },
  {
    "objectID": "posts/waiting_time/waitingqmd.html#introduction",
    "href": "posts/waiting_time/waitingqmd.html#introduction",
    "title": "Exponential waiting time",
    "section": "",
    "text": "When you are diving in to a mathematical modeling paper with compartmental modeling, the authors may state that the waiting time within a compartment is exponentially distributed. This only comes up when the paper is pointing out how biologically unrealistic it is (another blog post). When I was an undergraduate, I was always puzzled by this! This concept of exponentially distributed dwelling-time is not intuitive without understanding some ordinary differential equation! Imagine a single compartment which we simply call A. The outflow out of the compartment is dependent on a constant rate \\(k\\) and the amount of individuals in A. So the easiest way to write this is:\n\\[\\frac{dA}{dt} = -kA. \\]\nThis equation means that the change in \\(A\\) over time is equal to \\(kA\\). This is a differential equation that we can actually solve analytically!\nThis is fairly simple to solve and let me show you the steps (honestly, I really recommend doing this yourself as practice!)\n\n\nImagine you have a stage A and there are individuals leaving it (note the hats and briefcases) and on the left, that is the solution of our differential equations. This is an exponential decay!\nSo now okay let’s look closer at the solution \\(A= A_0 exp(-kt)\\). First, we’re interested in how the proportion of individuals that stay in stage A change over time. To do this, we can divide the equation by A0 and get this:\n\\[\\frac{A}{A_0} = exp(-kt). \\]\nThis means that the proportion of individuals who are still in stage \\(A\\) can be represented by this exponential decay. Okay, what about the proportion leaving stage A? If 30 percent of the individuals are in stage A that means 70% left so the proportion leaving so \\(1 - \\frac{A}{A_0}\\) thus we get:\n\\[ 1- \\frac{A}{A0} = 1 -exp(-kt).\\]\nHm, that’s interesting! That looks exactly like the cumulative distribution function (CDF) of the exponential distribution! Remember that to get the probability distribution function (PDF), we just have find the derivative of the CDF:\n\\[\\frac{d}{dt} (1- \\frac{A}{A0}) = \\frac{d}{dt}(1 -exp(-kt)). \\]\nWhich means that the PDF is \\(k exp(-kt)\\)\nAnd if you remembered the mean of the exponential PDF is \\(\\frac{1}{k}\\) (We could have found the mean of the CDF, but I just thought it was easier).\nThat means that the mean time that individuals dwell in A is \\(\\frac{1}{k}\\).\nTherefore, for example if \\(k = \\frac{1}{2}\\) per day, then the mean time that an individual stay in \\(A\\) is \\(\\frac{1}{(1/2)}\\) is 2 days.\nHopefully, I hope this makes it clear using differential equations that there is an exponential distribution. The exponentially distributed waiting time is an assumption! It can be biologically unrealistic because it assumes that that most individuals leave the compartment almost instantly."
  },
  {
    "objectID": "posts/stochastic_sis/stochastic_sis.html#the-packages-that-you-need",
    "href": "posts/stochastic_sis/stochastic_sis.html#the-packages-that-you-need",
    "title": "Stochastic SIS model",
    "section": "",
    "text": "library(igraph)\nlibrary(ggplot2)\nlibrary(ggnetwork)\nlibrary(dplyr)\nlibrary(gganimate)\n\nThis is for my sake of actually learning how to make a stochastic SIS model."
  },
  {
    "objectID": "posts/stochastic_sis/stochastic_sis.html#the-code",
    "href": "posts/stochastic_sis/stochastic_sis.html#the-code",
    "title": "Stochastic SIS model",
    "section": "The Code",
    "text": "The Code\nThe Gillepsie method takes that master equation. Where we first simulate the time to the next event and figure out which event has occcured (infection or recovery). We use something called a Poisson process where we say there is a rate at which events happen but we want it to be random AND independent. The rate at which something happens in the system is what we described with the master equation. The poisson process is also useful because we also know the time to events.\nPoisson (P_n) then the exponential waiting time is \\((1- exp(P_n)t)\\).\nWe can use a uniform distribution through something called:Inverse Transform Sampling.\n\n# We create a susceptible and infectious vector and we will simulate for a 100 time steps\nS = matrix(c(10,rep(0,100-1)), nrow = 100, ncol = 1 ) #susceptible individuals \nI = matrix(c(2,rep(0,100-1)), nrow = 100,ncol = 1) #infectious individuals\n\nHere are the parameters:\n\nbeta = 0.1\ngamma = 1/3\ntime = 0 \n\n\nfor (i in seq(1,100)){\n  \n  infection = beta * S[i] * I[i]\n  recovery = gamma * I[i]\n  total_rate = infection + recovery\n  \n  random_number&lt;- runif(2,min=0,max=1)\n  \n  \n  tau &lt;- (1 /total_rate) * log(1 / random_number[1])\n  \n  time = time +  tau\n  \n  rate_of_interest &lt;- infection/total_rate\n  \n  if(random_number[2] &lt; rate_of_interest){\n    \n    S[i+1] = S[i] - 1 \n    I[i+1] = I[i] + 1\n  }\n  else{\n     S[i+1] = S[i] + 1 \n     I[i+1] = I[i] - 1  \n    \n  }\n}\n\n\nfull_model&lt;- cbind.data.frame(time = seq(0,100),Sus = S, Infect = I)\n\n\nggplot(full_model, aes(x = time, y = S))+ geom_line(color = 'red') + \n  geom_line(aes(x = time, y= I),color = 'black')+theme_bw()"
  },
  {
    "objectID": "posts/stochastic_sis/stochastic_sis.html#doing-this-on-a-network",
    "href": "posts/stochastic_sis/stochastic_sis.html#doing-this-on-a-network",
    "title": "Stochastic SIS model",
    "section": "Doing this on a network",
    "text": "Doing this on a network\n\n#let's say there are 10 individuals \n\ngraph = erdos.renyi.game(25,50 , type = \"gnm\", directed = FALSE)\nV(graph)$label &lt;- seq(1,25)\nstate = rep(\"S\",25)\npatient_zero = sample(seq(1,25),1)\nstate[patient_zero] &lt;- \"I\"\n\n time_list = NULL\nfor (i in seq(1,100)){\n\n  infection_propensity = NULL\n  recovery_propensity = NULL\n\n  inf_node &lt;- which(state == \"I\")\n    \n  \n    recovery_propensity &lt;- list(cbind(n=  inf_node,rate=gamma))\n    neighbors &lt;-  neighbors(graph, inf_node )\n     \n     for (n in neighbors){\n        if (state[n] == \"S\") {\n         infection_propensity[[n]] &lt;- cbind(n,rate=beta)\n       \n        }\n       \n     }\n    \n    \n    full_infect_rate &lt;- cbind.data.frame(do.call(rbind,infection_propensity),status = 'infect')\n    full_recovery_rate &lt;- cbind.data.frame(do.call(rbind,recovery_propensity), status = 'recover')\n\n    events = rbind(full_infect_rate, full_recovery_rate)\n    \n     \n    total_rate &lt;- sum(events$rate)\n    \n     random &lt;- runif(2)\n    \n      tau &lt;- -log(random[1]) / total_rate\n     \n      cumulative_rate &lt;- 0\n      \n      selected_event &lt;- NULL\n      for (event in seq(1,nrow(events))) {\n      cumulative_rate &lt;- cumulative_rate + events$rate[event]\n      \n      if (random[2] * total_rate &lt;= cumulative_rate) {\n        selected_event &lt;- event\n        break\n      }\n    }\n       time &lt;- time + tau\n      \n      if (!is.null(selected_event)) {\n      node &lt;- selected_event\n      if (state[node] == \"I\") {\n        # Recovery event\n        state[node] &lt;- \"S\"\n      } else {\n        # Infection event\n        state[node] &lt;- \"I\"\n      }\n    }\n \n       time_list[[i]] = cbind.data.frame(time,state,node = seq(1,100))\n}      \n\n full_time_list &lt;- do.call(rbind, \n time_list )"
  },
  {
    "objectID": "posts/stochastic_sis/stochastic_sis.html#animating-it",
    "href": "posts/stochastic_sis/stochastic_sis.html#animating-it",
    "title": "Stochastic SIS model",
    "section": "Animating it",
    "text": "Animating it\n\nfull_graph&lt;- fortify(graph)\nfull_spatial &lt;-full_graph[,c('x','y','label')]\nfull_spatial&lt;- full_spatial[!(duplicated(full_spatial)), ]\n\neep&lt;- left_join( full_time_list ,full_spatial, by = join_by(node==label))\n\ngg_anim &lt;- ggplot(data=full_graph)+\n  geom_edges(aes(x = x, y = y, xend = xend, yend = yend),color = \"black\")+\n  geom_point(data = eep, aes(x =x, y=y,color = state),size =10)+\n   transition_states(time,  state_length = 1)+\n  enter_fade() + \n  exit_shrink() +\n  ease_aes('sine-in-out',interval = 0.001)+theme_void()\n\n  animate(gg_anim, fps=5)\n\n\n\n\n\n\n\n anim_save ('gganim.gif')"
  },
  {
    "objectID": "posts/predator_prey/predator-prey.html",
    "href": "posts/predator_prey/predator-prey.html",
    "title": "Guest lecture slides for teaching the Lotka-Volterra Predator/Prey dynamics",
    "section": "",
    "text": "Copyright 2024, Damie Pak"
  },
  {
    "objectID": "posts/secondary_vectors/secondary_vector.html",
    "href": "posts/secondary_vectors/secondary_vector.html",
    "title": "Secondary vectors: when David becomes Goliath",
    "section": "",
    "text": "With climate change, a major concern is about an increase of vector borne diseases. There’s been a lot of research that explores how increasing temperatures could increase transmission thus increasing the number of secondary cases. However, many of these papers focus on a single primary vector species. In certain disease systems, the pathogens can be transmitted by multiple vector species. For example, while the primary vector species of human malaria cases is Anopheles gambiae, there are secondary vectors that may contribute less to transmission but have significant epidemiological importance.\nOne thing that I think is really missing in getting prepared for the changing future is how “secondary vectors” can become bigger problems. Specifically, I’m interested in scenarios where human interventions can inadvertently change the vector community."
  },
  {
    "objectID": "posts/secondary_vectors/secondary_vector.html#introduction",
    "href": "posts/secondary_vectors/secondary_vector.html#introduction",
    "title": "Secondary vectors: when David becomes Goliath",
    "section": "",
    "text": "With climate change, a major concern is about an increase of vector borne diseases. There’s been a lot of research that explores how increasing temperatures could increase transmission thus increasing the number of secondary cases. However, many of these papers focus on a single primary vector species. In certain disease systems, the pathogens can be transmitted by multiple vector species. For example, while the primary vector species of human malaria cases is Anopheles gambiae, there are secondary vectors that may contribute less to transmission but have significant epidemiological importance.\nOne thing that I think is really missing in getting prepared for the changing future is how “secondary vectors” can become bigger problems. Specifically, I’m interested in scenarios where human interventions can inadvertently change the vector community."
  },
  {
    "objectID": "posts/secondary_vectors/secondary_vector.html#chagas-disease",
    "href": "posts/secondary_vectors/secondary_vector.html#chagas-disease",
    "title": "Secondary vectors: when David becomes Goliath",
    "section": "Chagas disease",
    "text": "Chagas disease\nSo my current postdoctoral work with Tad Dallas at the University of South Carolina emerged from a tiny workshop with South American scholars. I was watching some talks of Chagas disease and I was really fascinated with the system. All triatomine species can vector the parasite (Trypansoma cruzi) but their importance in transmission is based on their ecological traits. For example, an important vector species is Triatoma infestans and is known to reside in buildings. What I thought was really cool, is they competitively displace other species.\nIn the late 20th century, South American countries all allied together to rid the scourge of Chagas disease (The Southern Cone Initiative). Large-scale insecticidal spraying across the continent drove down T. infestans number. It was a successful campaign. However, post-campaign, there was a resurgence in secondary pest because the dominant competitor was eliminated. Additionally, the less important secondary pests are also less affected by the indoor insecticidal spraying because they were in the peridomestic or sylvatic ecotypes. Therefore, the new open niches were ripe for invasion by the secondary vectors.\nAnd the cool thing with Chagas disease is that the shift in the dominant vectors have happened multiple times. When the scientist Carlos Chagas was the one to discover the parasite, the dominant vector at the time was Panstrongylus megistus. Soon, it would be displaced by T. infestans."
  },
  {
    "objectID": "posts/secondary_vectors/secondary_vector.html#thoughts",
    "href": "posts/secondary_vectors/secondary_vector.html#thoughts",
    "title": "Secondary vectors: when David becomes Goliath",
    "section": "Thoughts",
    "text": "Thoughts\nOne thing that I really want to pursue for my research program is on how the vector composition can change over time and what that means for disease. It’s something that I really like thinking about when I was doing my tick paper. It’ll be cool if I type this up as a review paper on secondary vectors. What are the different mechanisms that can lead to a shift. I can imagine changes to the environment (urbanization) as well as control efforts."
  },
  {
    "objectID": "posts/abouheif_mean_C/abouheifqmd.html#introduction",
    "href": "posts/abouheif_mean_C/abouheifqmd.html#introduction",
    "title": "Abouheif Mean C",
    "section": "",
    "text": "A very silly thing I made to understand Abouheif Mean C (a non-complicated way to see if there is a phylogenetic signal). Sometimes to fully learn something, I need to do it tediously."
  },
  {
    "objectID": "posts/abouheif_mean_C/abouheifqmd.html#a-rambling",
    "href": "posts/abouheif_mean_C/abouheifqmd.html#a-rambling",
    "title": "Abouheif Mean C",
    "section": "A rambling",
    "text": "A rambling\nSo when we are doing some good ol’ linear regression, a critical assumption is that that the observations are independent of each other. However, what if we are doing a regression of Trait A and Trait B with each observation being a unique species? The problem is that species aren’t really independent from each other. Specifically, we can rightfully assume that species that are more genetically related to each other to be more similar. Therefore, there is no independence! Violation!\n\nOne way to test if there is a phylogenetic signal than in a trait before doing more complicated statistics that account for the correlation between species is by using the value Abouheif C. I’m following it from the original 1999 paper which I recommend a good read if you are a beginner of phylogenetic comparison like me. He gives a very clear flowchart of how to approach this phylogenetic correlation conundrum. Specifically, use a test statistics to see if there is a phylogenetic signal. If not! Then just use regular statistics. However, if there is a phylogenetic signal then do your more complicated method. \nSo in Abouheif’s original 1999 paper, he calls the test-statistic the test for serial-independence (TSI) which originated from famed mathematician von Neumann in his 1941 paper. As an aside, this is a very convoluted math paper and all I got from it is that it test for non-randomness in a series of continuous variates (cool, I guess). So we can kinda see the inkling of how this would help test for phylogenetic autocorrelation as we’re trying to figure out if there is (non)randomness in a series of observation that is structured by the phylogenetic tree.\nSo if the test-statistic is positive, it means evidence of phylogenetic descent (cannot find a good definition, but I’m assuming that the observed trait is due to similarity of descent?) while a negative statistic means evidence of convergent evolution (trait that evolved independently).\n\nSo to calculate the TSI you don’t have to know the branch length, but you have to understand the topology (aka the order in the sense of the von Neumann’s paper). \nThe equation is that it is the sum of the successive squared differences in between observations:\n\\(\\sum d^2 = \\sum(Y_{I+1} - Y_{I})\\) where \\(Y\\) is the observation of some trait we’re interested. If the observation are independent, then \\(d^2\\) will be twice the sum of squares: \\(\\sum y^2 = \\sum(Y_{I} - \\overline{Y})^2\\). Then for some weird magic, I guess the ratio \\(n = \\sum d^2/\\sum y^2\\) will aprproximate 2. Positive correlation would be that n is less than 2 and the variance of the successive difference will be less than if the observations were ordered randomly. Negative correlation is that the n is greater than 2 and the variance of the successive difference will be greater than if the observations were ordered randomly.\nOK than god that he includes an example of how to do this.\nThe example that he sets out is to discover if there is a phylogenetic signal with the body mass across eight species distributed across the phylogenetic tree. The hull hypothesis (\\(H_0\\)) is that there is serial independence and there is no correlation among species for body size. The alternative hypothesis (\\(H_a\\)) is that there is serial autocorrelation in body size. We then calculate the sequence of the observed trait (the body mass that is ordered on the phylogenetic tree). I made my messy sketch version."
  },
  {
    "objectID": "posts/abouheif_mean_C/abouheifqmd.html#the-sketch",
    "href": "posts/abouheif_mean_C/abouheifqmd.html#the-sketch",
    "title": "Abouheif Mean C",
    "section": "The Sketch",
    "text": "The Sketch\n\n\n\n\n\n \n\n \n\n  \n    \n       \n       \n    \n       \n    \n    \nCopyright 2024, Damie Pak"
  },
  {
    "objectID": "posts/EVPI/evpi.html",
    "href": "posts/EVPI/evpi.html",
    "title": "Expected value of perfect information",
    "section": "",
    "text": "This is a simple schematic I made to teach people the Value of Information. This concept emerges from decision theory and asks the question: “What is the benefits of resolving uncertainty?”"
  },
  {
    "objectID": "posts/EVPI/evpi.html#introduction",
    "href": "posts/EVPI/evpi.html#introduction",
    "title": "Expected value of perfect information",
    "section": "",
    "text": "This is a simple schematic I made to teach people the Value of Information. This concept emerges from decision theory and asks the question: “What is the benefits of resolving uncertainty?”"
  },
  {
    "objectID": "posts/EVPI/evpi.html#figure",
    "href": "posts/EVPI/evpi.html#figure",
    "title": "Expected value of perfect information",
    "section": "Figure",
    "text": "Figure"
  },
  {
    "objectID": "posts/compound_probability/compound_probability.html",
    "href": "posts/compound_probability/compound_probability.html",
    "title": "Compound probability and demographic stochasticity",
    "section": "",
    "text": "I am currently working on a group lab project and my role is adding demographic stochasticity to the Hassell population growth model. I haven’t really worked with stochasticity before, so a lot of my work is following Melbourne and Hastings 2008[1] . In their supplementary material, they explain how stochasticity is incorporated. The first model, the Ricker Poisson model, only includes demographic stochasticty.[1] Melbourne, Brett A., and Alan Hastings. “Extinction risk depends strongly on factors contributing to stochasticity.” Nature 454.7200 (2008): 100-103."
  },
  {
    "objectID": "posts/compound_probability/compound_probability.html#introduction",
    "href": "posts/compound_probability/compound_probability.html#introduction",
    "title": "Compound probability and demographic stochasticity",
    "section": "",
    "text": "I am currently working on a group lab project and my role is adding demographic stochasticity to the Hassell population growth model. I haven’t really worked with stochasticity before, so a lot of my work is following Melbourne and Hastings 2008[1] . In their supplementary material, they explain how stochasticity is incorporated. The first model, the Ricker Poisson model, only includes demographic stochasticty.[1] Melbourne, Brett A., and Alan Hastings. “Extinction risk depends strongly on factors contributing to stochasticity.” Nature 454.7200 (2008): 100-103."
  },
  {
    "objectID": "posts/compound_probability/compound_probability.html#the-number-of-eggs-is-a-poisson-process",
    "href": "posts/compound_probability/compound_probability.html#the-number-of-eggs-is-a-poisson-process",
    "title": "Compound probability and demographic stochasticity",
    "section": "The number of eggs is a Poisson process",
    "text": "The number of eggs is a Poisson process\nLike Melbourne and Hastings, we are modeling flour beetles. For an adult \\(i\\), we can express the number of eggs (\\(B\\)) laid in its lifetime at time \\(t\\) as:\n\\[\nB_{i,t} \\sim Poisson(\\beta),\n\\]\nwhere \\(\\beta\\) is the mean number of births.\nWhat would this look like in code? We can use rpois to generate the number of eggs. On average, a flour beetle lays 30 eggs in its lifetime (\\(\\beta = 30\\)). Let’s say there are 50 female adults that all lay eggs.\n\nnumber_of_adults = 50 #50 adults\nnumber_of_eggs &lt;- rpois(number_of_adults, lambda = 30) #Random amount of eggs\nfull_eggs_df &lt;- data.frame(adult_id = 1:50, number_of_eggs)\n\nWe can then visualize how much eggs each adult produced:\n\n\n\n\n\n\n\n\n\nAnd we can visualize this as a distribution:\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "posts/compound_probability/compound_probability.html#survival-of-the-eggs",
    "href": "posts/compound_probability/compound_probability.html#survival-of-the-eggs",
    "title": "Compound probability and demographic stochasticity",
    "section": "Survival of the eggs",
    "text": "Survival of the eggs\nThe number of eggs surviving to adulthood is also influenced by stochasticty. I am only assuming density-independent mortality which I call \\(\\mu\\) . If we assume that 25% of the individuals die (so 75% survivorship), then we can express the surviving eggs in the form of the binomial probability distribution. Specifically, of the offspring of adult \\(i\\), how many of them are going to survive to produce their own eggs?In the paper, they actually include egg cannibalism by adults\nThe binomial probability requires the number of trials (the offsprings) and the probability of survival. Basically, imagine that for each offspring, we are flipping a biased coin. The equation would then look like:\n\\[\nS_{i,t+1} \\sim Binomial(B_{i,t}, (1- \\mu) ),\n\\] where \\(S\\) is the surviving offsprings of adult \\(i\\).\nWe can code this by using a for-loop:\n\nfor (a in seq(1,50)){\nfull_eggs_df$survived[a] &lt;- rbinom(n = 1, size = full_eggs_df$number_of_eggs[a], prob = 0.75)\n}\n\n###survival rate\nfull_eggs_df$surv_rate &lt;-full_eggs_df$survived/full_eggs_df$number_of_eggs \n\nThis plot shows that for each adult i who produced \\(B\\) amount of eggs (grey bar), what are the proportion that survived? (black bar)"
  },
  {
    "objectID": "posts/compound_probability/compound_probability.html#compound-distribution",
    "href": "posts/compound_probability/compound_probability.html#compound-distribution",
    "title": "Compound probability and demographic stochasticity",
    "section": "Compound distribution",
    "text": "Compound distribution\nInterestingly, if we write the entire binomial distribution out:\n\\[\nS_{i,t+1} \\sim Binomial(Poisson(\\beta), (1- \\mu)),\n\\] we can see this as a compound distribution or more specifically, a compound Binomial-Poisson distribution. We can reduce that gnarly equation. Let’s find the expected number of surviving offspring which we call \\(\\mathbb{E}[S]\\). Let us also find the expected number of eggs laid which is \\(\\mathbb{E}[Poisson(\\beta)]\\). The mean of the Poisson distribution is just the parameter or \\(\\beta\\). Therefore:\n\\[\n\\mathbb{E} [S_{i,t+1}] \\sim Binomial(\\beta, (1- \\mu)),\n\\]\nThe average number of successes or survival for a binomial distribution is simply \\(np\\) where \\(n\\) is the number of trials and \\(p\\) is the probability of success. In other words \\(n = \\beta\\) and \\(p = (1- \\mu)\\)\nTherefore, \\(\\mathbb{E}[S] = \\beta (1- \\mu)\\). This is saying the average number of eggs that will survive to lay their own eggs in the next generation is \\(\\beta (1 - \\mu)\\). Because this is the average, we can then use the Poisson distribution (Remember, the expectation of the Poisson distribution is its parameter). Specifically:\n\\[\nS_{i,t+1}  \\sim Poisson(\\beta(1- \\mu)).\n\\] The total number of adults in the next generation is then:\n\\[\nN_{t+1} = \\sum_{i=1}^{N_t} S_{i,t+1}.\n\\]\nWe can then make it simpler because of the rule that states:\n\nLet \\(X \\sim Poisson(\\lambda_1)\\) and let \\(Y \\sim Poisson(\\lambda_2)\\), their sum is a Poisson distribution and \\(X+Y \\sim (\\lambda_1 + \\lambda_2)\\)\n\nThat means that if we get the total average (or \\(N_t (1- \\mu)\\)) we can simplify this to:\n\\[\nN_{t+1} \\sim Poisson (N_t (1- \\mu)).\n\\] TA-DA this is how they got it!"
  },
  {
    "objectID": "posts/Where do 'failed ideas' go?/badideas.html",
    "href": "posts/Where do 'failed ideas' go?/badideas.html",
    "title": "What is the point of mathematical models in ecology?",
    "section": "",
    "text": "Sometimes we work with something so much, we need to take a step back and ask: “What’s the point?” I spent about a decade doing research in ecology! As a generalist modeler, I worked with different kinds of model. Yet, if someone asks me: “What is the point of models in ecology?”, what am I suppose to say? I would flippantly argue: “It’s for people who is absolutely ass at hte"
  },
  {
    "objectID": "posts/Where do 'failed ideas' go?/badideas.html#introduction",
    "href": "posts/Where do 'failed ideas' go?/badideas.html#introduction",
    "title": "What is the point of mathematical models in ecology?",
    "section": "",
    "text": "Sometimes we work with something so much, we need to take a step back and ask: “What’s the point?” I spent about a decade doing research in ecology! As a generalist modeler, I worked with different kinds of model. Yet, if someone asks me: “What is the point of models in ecology?”, what am I suppose to say? I would flippantly argue: “It’s for people who is absolutely ass at hte"
  },
  {
    "objectID": "posts/cool_stuff_1/coolstuff1.html",
    "href": "posts/cool_stuff_1/coolstuff1.html",
    "title": "Cool things (Week of October 15)",
    "section": "",
    "text": "I decided it’s easier to make the papers I read or cool things I learned into one big list. Most if it is work stuff, but there are lot of personal things."
  },
  {
    "objectID": "posts/cool_stuff_1/coolstuff1.html#preamble",
    "href": "posts/cool_stuff_1/coolstuff1.html#preamble",
    "title": "Cool things (Week of October 15)",
    "section": "",
    "text": "I decided it’s easier to make the papers I read or cool things I learned into one big list. Most if it is work stuff, but there are lot of personal things."
  },
  {
    "objectID": "posts/cool_stuff_1/coolstuff1.html#the-infinite-weight-of-mediocrity",
    "href": "posts/cool_stuff_1/coolstuff1.html#the-infinite-weight-of-mediocrity",
    "title": "Cool things (Week of October 15)",
    "section": "The Infinite Weight of Mediocrity",
    "text": "The Infinite Weight of Mediocrity\nA beautiful video essay that I watched about being an artist. It hits close to being an academic though. The author concludes that self-doubt and disappointment is perhaps the norm if you’re creating something."
  },
  {
    "objectID": "posts/cool_stuff_1/coolstuff1.html#theory-and-models-in-ecology-a-different-perspective",
    "href": "posts/cool_stuff_1/coolstuff1.html#theory-and-models-in-ecology-a-different-perspective",
    "title": "Cool things (Week of October 15)",
    "section": "Theory and models in ecology: A different perspective",
    "text": "Theory and models in ecology: A different perspective\n(Caswell 1988 in Ecological Modeling)\nI’m thinking more about myself and my field lately. As grad students, we were never forced to think broadly about our field because we’re supposed to be focused on technical details. I think most PI’s care about their student something specific to the grand. I really enjoyed this paper because it’s a simple explanation of what the point of theory and models are. My favorite quote and maybe how I will always frame my project: What is the simplest model which can predict the observed phenomenon?"
  },
  {
    "objectID": "posts/theory/changebacktime.html",
    "href": "posts/theory/changebacktime.html",
    "title": "What is the point of journal club?",
    "section": "",
    "text": "I am very notorious for disliking journal clubs. I had both my peers and superiors try to guilt me into me into it. I complain! People point out about how I met my wife at journal club. It does not matter! Here are some rambling thoughts about journal clubs I liked and loathed."
  },
  {
    "objectID": "posts/theory/changebacktime.html#preamble",
    "href": "posts/theory/changebacktime.html#preamble",
    "title": "What is the point of journal club?",
    "section": "",
    "text": "I am very notorious for disliking journal clubs. I had both my peers and superiors try to guilt me into me into it. I complain! People point out about how I met my wife at journal club. It does not matter! Here are some rambling thoughts about journal clubs I liked and loathed."
  },
  {
    "objectID": "posts/theory/changebacktime.html#dont-guilt-me-into-it",
    "href": "posts/theory/changebacktime.html#dont-guilt-me-into-it",
    "title": "What is the point of journal club?",
    "section": "1. Don’t guilt me into it!",
    "text": "1. Don’t guilt me into it!\nDon’t use guilt to ‘persuade’ people into your journal club. Especially if you’re guilting a postdoc. It makes us feel like we’re not doctorate holding individuals but some graduate student who doesn’t know better. N"
  },
  {
    "objectID": "posts/theory/changebacktime.html#no-morning-journal-club",
    "href": "posts/theory/changebacktime.html#no-morning-journal-club",
    "title": "What is the point of journal club?",
    "section": "2. No morning journal club!",
    "text": "2. No morning journal club!\nOne thing I love about my current journal club is that it’s at about 3:00 PM. That’s the nadir of my productivity in any working day. Mornings are actually when I work! I had a 9 AM journal club that I skipped for good reason!"
  },
  {
    "objectID": "posts/theory/changebacktime.html#only-have-one-faculty-member",
    "href": "posts/theory/changebacktime.html#only-have-one-faculty-member",
    "title": "What is the point of journal club?",
    "section": "3. Only have one faculty member",
    "text": "3. Only have one faculty member\nI like the idea of a grad student lead journal club in theory. In practice, they rarely last. Having one faculty lead it is great. Maybe a PI that is more laidback and chill. The journal club that I met my wife in had a very chill math professor.\nI went to a journal club with like 3 professors and that was pushing it. It felt like journal club was their conversations, and the postdocs and grad students were their audience."
  },
  {
    "objectID": "posts/theory/changebacktime.html#allow-for-non-technical-science-papers",
    "href": "posts/theory/changebacktime.html#allow-for-non-technical-science-papers",
    "title": "What is the point of journal club?",
    "section": "4. Allow for non-technical science papers",
    "text": "4. Allow for non-technical science papers\nI’ve been really into these very broad science papers that doesn’t have much technical details. I think once every 4 weeks, these types of papers should be discussed."
  },
  {
    "objectID": "posts/theory/changebacktime.html#dont-choose-a-programming-book",
    "href": "posts/theory/changebacktime.html#dont-choose-a-programming-book",
    "title": "What is the point of journal club?",
    "section": "5. Don’t choose a programming book",
    "text": "5. Don’t choose a programming book\nI was at a journal club where we had to read a programming book. I don’t know. It didn’t go well. Especially because doing the meeting, most people didn’t have their laptops out."
  },
  {
    "objectID": "posts/theory/changebacktime.html#dont-have-those-introductory-presentations",
    "href": "posts/theory/changebacktime.html#dont-have-those-introductory-presentations",
    "title": "What is the point of journal club?",
    "section": "6. Don’t have those introductory presentations",
    "text": "6. Don’t have those introductory presentations\nI was at a journal club where everyone had to make slides on their papers before discussing it. I didn’t like it. Like journal club is supposed to be this casual thing where we discuss."
  },
  {
    "objectID": "posts/theory/changebacktime.html#choose-a-specific-theme-or-topic",
    "href": "posts/theory/changebacktime.html#choose-a-specific-theme-or-topic",
    "title": "What is the point of journal club?",
    "section": "7. Choose a specific theme or topic",
    "text": "7. Choose a specific theme or topic\nI think journal club would be better if there is a specific theme. For example, I always wanted to host a journal club for the original series of paper that introduced the SIR model. It has some gnarly math (do you know that it was originally derived with partial differential equations) and it would be a cool trip back to the past."
  },
  {
    "objectID": "posts/cool_stuff_1/coolstuff1.html#how-to-write-an-overture-a-rossini-recipe",
    "href": "posts/cool_stuff_1/coolstuff1.html#how-to-write-an-overture-a-rossini-recipe",
    "title": "Cool things (Week of October 15)",
    "section": "How to Write an Overture: A Rossini Recipe",
    "text": "How to Write an Overture: A Rossini Recipe\n(Spike Hughes 1956 in the Musical Times)\nHilarious paper about Rossini responding to a fan-letter. Basically, the fan asks him: “How do you write an overture?” and Rossini is like: “Dude, I have to be forced into it. It should be written last minute.”\nI guess he had to be locked in a room and forced to toss out his overtures out the window for someone to collect."
  },
  {
    "objectID": "posts/dissectionposter/dissection.html",
    "href": "posts/dissectionposter/dissection.html",
    "title": "A dissection of some posters I made in my last postdoc",
    "section": "",
    "text": "It’s good to look back and cringe. It’s how we learn I guess. Here are two posters I made as a postdoc at Cornell."
  },
  {
    "objectID": "posts/dissectionposter/dissection.html#introduction",
    "href": "posts/dissectionposter/dissection.html#introduction",
    "title": "A dissection of some posters I made in my last postdoc",
    "section": "",
    "text": "It’s good to look back and cringe. It’s how we learn I guess. Here are two posters I made as a postdoc at Cornell."
  },
  {
    "objectID": "posts/dissectionposter/dissection.html#eeid-2022",
    "href": "posts/dissectionposter/dissection.html#eeid-2022",
    "title": "A dissection of some posters I made in my last postdoc",
    "section": "EEID 2022",
    "text": "EEID 2022\n\nI told my postdoc advisor that I wanted my poster to be “Hot Girl Summer” (Credit to Megan Thee Stallion). I’m not a fan of the quarter circle at all. Yes, it’s eye catching, but it makes the white space almost awkward to use. In some ways, I think it makes it more non-functional. Look at me trying to fit the very rectangular plots in there! Notice the awkward placement of the legend in the bottom! Also with the circular design, it’s hard to figure out how to read the poster. It’s not a very logical, intuitive flow.\nShockingly, I also cannot believe that I put yellow on white - a design faux pas! I appreciate design, and I think that it is important for communicating ideas. However, design can definitely get in the way, and I think that is the case here. It’s vibrant and colorful, but I think I needed to be more ‘grounded’."
  },
  {
    "objectID": "posts/dissectionposter/dissection.html#eeid-2023",
    "href": "posts/dissectionposter/dissection.html#eeid-2023",
    "title": "A dissection of some posters I made in my last postdoc",
    "section": "EEID 2023",
    "text": "EEID 2023\n\nI then told my postdoc advisor that I wanted my poster to give “synthwave summer”. I want to say that even if it’s the same research, I got some new results! The format is more grounded- you go from top to bottom. There’s a common rule that says not to have gradients as the background of the poster- but I am happy to break that rule. I think it’s subtle, and that’s the key to making it look good. I’m happier with the introduction/methods part of my poster.\nMy biggest regret is the last two plots at the bottom. My wife called it the “wiener dog plots” due to how long they are. In my head, I thought it was going to scream “Look at me! Look at me!” But they look very, very awkward. This could be easily fixed by reducing the width.\nOverall, I’m happy with the look of this poster. I spent two days on it."
  },
  {
    "objectID": "competitio.html",
    "href": "competitio.html",
    "title": "Lotka-Volterra Competition Model in Motion",
    "section": "",
    "text": "I’m always astounded when revisiting the foundations of my field. The first time I saw the Lotka-Volterra competition model was in my Introduction to Ecology class. I come across it in passing when reading papers, and I see it more frequently now that I’m tackling more community ecology questions. Specifically, I am interested in the role of competition between vector species and how it influences disease risk. It’s always good to return to the basics to see how much you’ve grown as a researcher.\nI think one way to demonstrate a deep understanding of the material is by making figures! While learning the mathematics is crucial, I find it helpful to code along with it. Also, if you’re interested in animating something like the Lotka-Volterra competition model, I’ve got you covered.\n\nlibrary(ggplot2)\nlibrary(gganimate)\nlibrary(reshape2)\nlibrary(viridis)"
  },
  {
    "objectID": "competitio.html#introduction",
    "href": "competitio.html#introduction",
    "title": "An introduction to Lotka-Volterra Competition Model",
    "section": "Introduction",
    "text": "Introduction\nI’m always astounded going back to the foundations of my field. I saw the Lotka-Volterra competition model in my Introduction to Ecology class. Didn’t think about it that much, and now it emerges as part of my vector control project. I wanted to brush-up my foundations, and why not make some figures!"
  },
  {
    "objectID": "competitio.html#the-lotka-volterra-competition-model",
    "href": "competitio.html#the-lotka-volterra-competition-model",
    "title": "An introduction to Lotka-Volterra Competition Model",
    "section": "The Lotka-Volterra Competition model",
    "text": "The Lotka-Volterra Competition model\nThe Lotka-Volterra (LV) competition model is the seminal work that describes the dynamics of competing species. For non-ecologists, let me tell you that this is a key equation. Assume that we have two species: \\(N_1\\) and \\(N_2\\), let us assume they have the same growth rate: The equations are:\n\\[\n\\frac{dN_1}{dt} = rN_1(1- (c_{11} N_1 + c_{21}N_2))\n\\]\n\\[\n\\frac{dN_2}{dt} = rN_2(1- (c_{22} N_2 + c_{12}N_1)).\n\\]\nHere, the \\(c\\) is the competition coefficient that describes how much members of it’s own species (\\(c_{11} and c_{22}\\)) regulate its growth rate as well as that of the other species: \\(c_{12}\\) and \\(c_{21}\\)). You can read the last two terms as: the competitive effect of species 1 on species 2 and vice-versa. You have have seen different formulations, especially with an explicit carrying capacity (\\(K\\)) but I just absorbed it in the competition coefficients.\\(rN_1(1-\\frac{(N_1 - c_{21}N_2)}{K}))\\)\nSo remember:\n\n\\(c_11\\) is the competition between species 1\n\\(c_22\\) is the competition between species 2\n\\(c_12\\) is the competition of species 1 on species\n\\(c_21\\) is the competition of species 2 on species\n\nWhy don’t we make a simulation function for it:\n\nLV_func &lt;- function(c_df, P0, S0, timestep){\n  \n  pop_mat &lt;- matrix(0, nrow = timestep, ncol = 3)\n  pop_mat[, 1] &lt;- seq(1,timestep)\n  pop_mat[1,2] &lt;- P0 #initial first population\n  pop_mat[1,3] &lt;- S0 #initial second population\n  \n  ###Competition coefficient\n  c_11 &lt;- c_df[\"c_11\"] \n  c_12 &lt;- c_df[\"c_12\"] \n  c_22 &lt;- c_df[\"c_22\"] \n  c_21 &lt;- c_df[\"c_21\"] \n\n  r = 0.01\n  \n  for (t in seq(1,timestep - 1)){\n    \n    P = pop_mat[t,2]\n    S = pop_mat[t,3]\n    \n    P_change &lt;- r * P * (1-(c_11*P + c_21*S))\n    S_change &lt;- r * S * (1-(c_22*S + c_12*P))\n    \n    pop_mat[t+1, 2] = as.numeric(P + P_change)\n    pop_mat[t+1, 3] = as.numeric(S + S_change)\n\n  }\n  return(pop_mat)\n}\n\n\nc_df &lt;- data.frame(c_11 = 5e-2, \n                   c_22 = 3e-2, \n                   c_12 = 5e-3,\n                   c_21 = 1e-5)\n\nLV_df &lt;- LV_func(c_df, 100, 100, 2000)\n\n\nplot_LV &lt;- function(mat){\n  \n  LV_df &lt;- data.frame(mat)\n  colnames(LV_df) &lt;- c(\"time\", \"N\", \"S\")\n  LV_melt &lt;- melt(LV_df, id.vars = 'time')\n  \n  GG&lt;- ggplot(LV_melt, \n         aes(x= time, y = value, color = variable)) + \n           geom_line() + \n           xlab(\"Time\") + \n           ylab(\"Abundance\") +\n           scale_color_manual(values = c(\"red\", \"blue\"), name = \"Species\")+ \n           theme_classic() + \n           theme(axis.text = element_text(size = 14),\n                 axis.title = element_text(size = 15))\n  \n         return(GG)\n}\n\n\nplot_LV (LV_df)\n\n\n\n\n\n\n\n\nCool, the parameters that we chose show that the two competing species are at equilibrium with \\(N_2\\) being at a higher equilibirum than \\(N_1\\). Play around with the code! A fun coding exercises is trying to figure out how to simulate a bunch of different competition coefficient and seeing what the ending equilibrium abundance is!"
  },
  {
    "objectID": "competitio.html#the-math-grind-of-finding-the-equilibrium",
    "href": "competitio.html#the-math-grind-of-finding-the-equilibrium",
    "title": "An introduction to Lotka-Volterra Competition Model",
    "section": "The math grind of finding the equilibrium:",
    "text": "The math grind of finding the equilibrium:\nRemember to find the equilbirum, we want to set both \\(\\frac{dN_1}{dt}\\) and \\(\\frac{dN_2}{dt}\\) to 0. The first equilibrium is when \\(N_1\\) is 0 and when \\(N_2\\) is 0. Not surprising, if you have nothing present, than nothing can happen.\nWe can also set \\(N_1\\) to 0 which then gives \\(N_2^* = \\frac{r}{c_{22}}\\) and when we set \\(N_2\\) to 0, this gives us \\(N_1^2 = \\frac{r}{c_{11}}\\). This means that in the absence of the other competitor, these species will reach their equilibrium that is dependent on the birth rate divided by the competition coefficient.\nThe last condition is then the coexistence\n\\[\nN_1^* = \\frac{c_{22} - c_{21}}{c_{22} c_{11} - c_{12}c_{21}}\n\\] and \\[\nN_2^* = \\frac{c_{11} - c_{12}}{c_{22} c_{11} - c_{12}c_{21}}\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\nSimple equations, but it did take me some time to calculate the equilibrium \\(N_1*\\) and \\(N_2^*\\). Good exerise in good organization and perhaps grouping and reassigning variables. Or you can brute-force it as I did."
  },
  {
    "objectID": "competitio.html#phase-plot",
    "href": "competitio.html#phase-plot",
    "title": "Lotka-Volterra Competition Model in Motion",
    "section": "Phase plot",
    "text": "Phase plot\nI don’t have a good intuition of equations without seeing it visually! One way to visualize what is going is using isoclines: contours that describe where for \\(N_1\\) and \\(N_2\\) the population does not change (\\(\\frac{dN_1}{dt} = 0\\) or \\(\\frac{dN_2}{dt} = 0\\))  If the isoclines cross, that is where coexistence is possible!Lotka, have beautiful isoclines\nWe can find the isoclines analytically! Basically, how the population of each species change is based on their own density as well as the other. So to analyze that visually, we should put both species on the axes: \\(N_1\\) on the x-axis and \\(N_2\\) on the y-axis. A visual representation of what I mean:\n\nslate &lt;- ggplot(data = NULL) + \n  xlab(expression(N[1])) + \n  ylab(expression(N[2])) + \n  theme_classic() +\n  theme(axis.text = element_text(size = 14, color=\"black\"),\n        axis.title = element_text(size = 15, color = \"black\"));\nslate\n\n\n\n\n\n\n\n\nTo find the isocline, set \\(\\frac{dN}{dt} = 0\\) and we get:\n\\[\n\\frac{1 -  c_{21} N_2}{c_{11}} =  N_1\n\\] By generating different abundances for \\(N_2\\), we can see what the \\(N_1\\) has to be for its isocline to be 0!\n\nisocline_N1 &lt;- function(c_df) {\n  ab &lt;- as.numeric(seq(0, 200))\n  c_11 &lt;- as.numeric(c_df[\"c_11\"])\n  c_21 &lt;- as.numeric(c_df[\"c_21\"])\n\n  isocline &lt;- (1 - (c_21 * ab)) / c_11\n\n  full &lt;- cbind.data.frame(N2 = ab, N1 = isocline)\n  return(full)\n}\n\nisoN1 &lt;- isocline_N1(c_df)\n\n\nslate + geom_line(data =isoN1,aes(x= N1, y= N2), color = \"#F36B23\") \n\n\n\n\n\n\n\n\nSame idea for the \\(N_2\\):\n\\[\nN_2  = \\frac{1- c_{12}N_1}{c_{22} }\n\\]\n\nisocline_N2 &lt;- function(c_df){\n  ab &lt;- as.numeric(seq(0,200))\n  c_22 &lt;- as.numeric(c_df[\"c_22\"])\n  c_12 &lt;- as.numeric(c_df[\"c_12\"])\n\n  isocline &lt;- (1 - (c_12 * ab))/c_22\n  \n  full &lt;- cbind.data.frame(N1 = ab,N2= isocline)\n  return(full)\n}\n\n\nisoN2 &lt;- isocline_N2(c_df)\nisocline_GG &lt;-  slate + \n  geom_line(data =isoN1,aes(x= N1, y= N2), color = \"#F36B23\") +\n  geom_line(data = isoN2, aes(x = N1, y= N2), color =\"#68B8BE\") ;\nisocline_GG\n\n\n\n\n\n\n\n\nWhere the isoclines cross is when \\(\\frac{dN_1}{dt}\\) and \\(\\frac{dN_2}{dt}\\) are 0 - that’s coexistence! Let’s check if our calculation of the coexistence equilibirum point is right? If it’s right, our point should be directly on where the isoclines cross.\n\ncalculate_coexistence &lt;- function(c_df){\n  c_11 &lt;- as.numeric(c_df[\"c_11\"])\n  c_12 &lt;- as.numeric(c_df[\"c_12\"])\n  c_22 &lt;- as.numeric(c_df[\"c_22\"])\n  c_21 &lt;- as.numeric(c_df[\"c_21\"])\n  \n  denom &lt;- (c_22 * c_11 - c_12 * c_21)\n  \n  N1_eq&lt;- (c_22 - c_21)/denom\n  N2_eq&lt;- (c_11 - c_12)/denom\n\n\n  return(cbind.data.frame(N1 = N1_eq, N2 = N2_eq))\n  }\n\n\ncoexist_df&lt;- calculate_coexistence (c_df)\n\n\nisocline_GG + geom_point(data = coexist_df, aes(x = N1, y= N2), size = 4)\n\n\n\n\n\n\n\n\nCool! But seems awfully empty."
  },
  {
    "objectID": "competitio.html#premise",
    "href": "competitio.html#premise",
    "title": "Lotka-Volterra Competition Model in Motion",
    "section": "",
    "text": "I’m always astounded when revisiting the foundations of my field. The first time I saw the Lotka-Volterra competition model was in my Introduction to Ecology class. I come across it in passing when reading papers, and I see it more frequently now that I’m tackling more community ecology questions. Specifically, I am interested in the role of competition between vector species and how it influences disease risk. It’s always good to return to the basics to see how much you’ve grown as a researcher.\nI think one way to demonstrate a deep understanding of the material is by making figures! While learning the mathematics is crucial, I find it helpful to code along with it. Also, if you’re interested in animating something like the Lotka-Volterra competition model, I’ve got you covered.\n\nlibrary(ggplot2)\nlibrary(gganimate)\nlibrary(reshape2)\nlibrary(viridis)"
  },
  {
    "objectID": "competitio.html#reminder-the-lotka-volterra-competition-model",
    "href": "competitio.html#reminder-the-lotka-volterra-competition-model",
    "title": "Lotka-Volterra Competition Model in Motion",
    "section": "Reminder: The Lotka-Volterra Competition model",
    "text": "Reminder: The Lotka-Volterra Competition model\nThe Lotka-Volterra (LV) competition model is a seminal work that describes the dynamics of two competing species. By varying the strength of competition—both within a species and between species—we can observe competitive exclusion (where one species wins) or coexistence.\nFor the equations, assume we have two species \\(N_1\\) and \\(N_2\\):\n\\[\n\\begin{aligned}\n\\frac{dN_1}{dt} &= rN_1\\left(1 - (c_{11} N_1 + c_{21}N_2)\\right) \\\\\n\\frac{dN_2}{dt} &= rN_2\\left(1 - (c_{22} N_2 + c_{12}N_1)\\right).\n\\end{aligned}\n\\]\nHere, \\(r\\) represents the intrinsic growth rate, and the \\(c\\) values are the competition coefficients, which describe how much members of a species regulate their own growth rate \\(c\\) are the competition coefficients that describe how much members of it’s own species (\\(c_{11} and c_{22}\\)) as well as the impact of the other species (\\(c_{12}\\) and \\(c_{21}\\)). You can interpret the last two terms as the competitive effect of species 1 on species 2 and the competitive effect of species 2 on species 1. As an aside, you may have seen different formulations, especially with an explicit carrying capacity (\\(K\\)) but I just absorbed it in the competition coefficients.A common formulation: \\(rN_1(1-\\frac{(N_1 - c_{21}N_2)}{K}))\\)\nSo remember:\n\n\\(c_{11}\\) is the competition between members of species 1\n\\(c_{22}\\) is the competition between members of species 2\n\\(c_{12}\\) is the competition of species 1 on species 2\n\\(c_{21}\\) is the competition of species 2 on species 1\n\nLet’s make a super simple function for simulating it- I’m just going to discretize the continuous model (honestly, should add a \\(\\delta_T\\))\n\n#c_df is a data.frame containing the competition coefficients above\n#N1_0 is the initial population for N1\n#N2_0 is the initial population for N2\n#timestep is how long we want ot run it for\n\n\nLV_func &lt;- function(c_df, N1_0, N2_0, timestep){\n  \n  #Generate a matrix to keep our records\n  pop_mat &lt;- matrix(0, nrow = timestep, ncol = 3)\n  pop_mat[, 1] &lt;- seq(1,timestep)\n  pop_mat[1,2] &lt;- N1_0 #initial first population\n  pop_mat[1,3] &lt;- N2_0 #initial second population\n  \n  ###Competition coefficient\n  c_11 &lt;- as.numeric(c_df[\"c_11\"])\n  c_12 &lt;- as.numeric(c_df[\"c_12\"])\n  c_22 &lt;- as.numeric(c_df[\"c_22\"])\n  c_21 &lt;- as.numeric(c_df[\"c_21\"])\n\n  r = 0.01 #Intrinsic growth rate is same for all species\n  \n  for (t in seq(1,timestep - 1)){\n    \n    N1= pop_mat[t,2] #Get the current population sizes\n    N2 = pop_mat[t,3]\n    \n    N1_change &lt;- r * N1 * (1-(c_11*N1 + c_21*N2))\n    N2_change &lt;- r * N2 * (1-(c_22*N2 + c_12*N1))\n    \n    pop_mat[t+1, 2] = N1 + N1_change\n    pop_mat[t+1, 3] = N2 + N2_change\n\n  }\n  return(pop_mat)\n}\n\nHere are the competition coefficients I chose:\n\nc_df &lt;- data.frame(c_11 = 1e-2, \n                   c_22 = 1e-2, \n                   c_12 = 5e-3,\n                   c_21 = 4e-3)\n\nLV_df &lt;- LV_func(c_df, 100, 100, 2000)\nhead(LV_df)\n\n     [,1]      [,2]      [,3]\n[1,]    1 100.00000 100.00000\n[2,]    2  99.60000  99.50000\n[3,]    3  99.20758  99.00947\n[4,]    4  98.82254  98.52815\n[5,]    5  98.44470  98.05581\n[6,]    6  98.07389  97.59222\n\n\n\nplot_LV &lt;- function(mat) {\n  LV_df &lt;- data.frame(mat)\n  colnames(LV_df) &lt;- c(\"time\", \"N1\", \"N2\")\n  LV_melt &lt;- melt(LV_df, id.vars = \"time\")\n\n  GG &lt;- ggplot(\n    LV_melt,\n    aes(x = time, y = value, color = variable)) +\n    geom_line() +\n    scale_color_manual(values = c(\"#F36B23\", \"#68B8BE\"), name = \"Species\") +\n    xlab(\"Time\") +\n    ylab(\"Abundance\") +\n    theme_classic() +\n    theme(\n      axis.text = element_text(size = 14, color = 'black'),\n      axis.title = element_text(size = 15,color = 'black')\n    )\n\n  return(GG)\n}\n\nLet’s plot it out:\n\nplot_LV(LV_df)\n\n\n\n\n\n\n\n\nCool! The parameters that we chose show that the two competing species are at equilibrium, with \\(N_1\\) being at a higher equilibrium than \\(N_2\\). Play around with the code! What parameters do you have to change to get competitive exclusion (one species winning it all?) A fun coding exercise is trying to figure out how to simulate a bunch of different competition coefficients and see what the ending equilibrium abundances are!"
  },
  {
    "objectID": "competitio.html#finding-the-equilibrium",
    "href": "competitio.html#finding-the-equilibrium",
    "title": "Lotka-Volterra Competition Model in Motion",
    "section": "Finding the equilibrium",
    "text": "Finding the equilibrium\nI always recommend finding the equilibrium once just to humble yourself. Remember to find the equilibrium, we want to set both \\(\\frac{dN_1}{dt}\\) and \\(\\frac{dN_2}{dt}\\) to 0. The first equilibrium is when \\(N_1\\) is 0 and when \\(N_2\\) is 0. Not surprising, if there are no species… nothing happens.\nWe can also set \\(N_1\\) to 0 which then gives \\(N_2^* = \\frac{r}{c_{22}}\\) and when we set \\(N_2\\) to 0, this gives us \\(N_1^* = \\frac{r}{c_{11}}\\). This means that in the absence of the other competitor, these species will reach their equilibrium that is dependent on the intrinsic growth divided by the intraspecific competition coefficient.\nThe last condition is then coexistence\n\\[\nN_1^* = \\frac{c_{22} - c_{21}}{c_{22} c_{11} - c_{12}c_{21}}\n\\] and \\[\nN_2^* = \\frac{c_{11} - c_{12}}{c_{22} c_{11} - c_{12}c_{21}}\n\\] This is interesting, basically. For the equilibrium abundances of both species \\(N_1^*\\) and \\(N_2^*\\) to be positive, the intrinsic growth rate of\n\n\n\n\n\n\nNote\n\n\n\n\n\nSimple equations, but don’t let it fool you! It did take some time to calculate the equilibrium \\(N_1*\\) and \\(N_2^*\\). Good exercise in good organization- though I think it would have been easier if I made new variables. Ah, but you can brute-force it as I did."
  },
  {
    "objectID": "competitio.html#adding-integral-curves",
    "href": "competitio.html#adding-integral-curves",
    "title": "Lotka-Volterra Competition Model in Motion",
    "section": "Adding integral curves",
    "text": "Adding integral curves\nWe can add integral curves to figure out where the initial conditions would take you. It can also give us some clue if that coexistence is stable or unstable! How about we animate it? I always think that anything dynamical should be animated. It’s the study of motions! I’m going to be using the {gganimate} package\nHere, I’m generating a dataframe where we have different combinations of the initial abundances of \\(N_1\\) and \\(N_2\\).\n\nN12_0 &lt;- expand.grid(N1= seq(1, 200, length = 15), \n                    N2 = seq(1, 200, length = 15))\n\nUsing our previous function LV_func, we can calculate the trajectory across time. I’m still not happy with this section of the code, and still figuring out how to make it cleaner (if you have any suggestions, pleaes email me!)\n\nN12_full &lt;- do.call(\n  rbind.data.frame,\n  apply(N12_0, 1, function(x) {\n    LV_func(\n      c_df,\n      x[\"N1\"],\n      x[\"N2\"], timestep = 700\n    )\n  },\n  simplify = FALSE\n  )\n)\n###Add an id\nN12_full$id &lt;- rep(seq(1, nrow(N12_0)), each = 700)\ncolnames(N12_full) &lt;- c(\"time\", \"N1\", \"N2\", \"id\")\n\n\nanim &lt;- slate + geom_path(\n  data = N12_full,\n  aes(x = N1, y = N2, group = id, color = time), size = 0.9, alpha = 0.8\n) +\n  geom_point(data = coexist_df, aes(x = N1, y = N2), size = 3) +\n  geom_line(data = isoN1, aes(x = N1, y = N2), color = \"#F36B23\", size = 1,alpha = 0.5) +\n  geom_line(\n    data = isoN2, aes(x = N1, y = N2), color = \"#68B8BE\",\n    size = 1, alpha = 0.5\n  ) +\n  theme_classic() +\n  scale_color_viridis(option = \"plasma\") +\n  transition_reveal(time) +\n  ease_aes(\"cubic-in-out\")\n\n\nanim"
  },
  {
    "objectID": "competitio.html#outro",
    "href": "competitio.html#outro",
    "title": "Lotka-Volterra Competition Model in Motion",
    "section": "Outro",
    "text": "Outro\nAnimating the LV model was super fun! Good way of learning for beginners!"
  },
  {
    "objectID": "competitio.html#isoclines",
    "href": "competitio.html#isoclines",
    "title": "Lotka-Volterra Competition Model in Motion",
    "section": "Isoclines",
    "text": "Isoclines\nI don’t have a good intuition of equations without seeing it visually! One way to visualize what is going is using isoclines: contours that describe where for \\(N_1\\) and \\(N_2\\) the population does not change (\\(\\frac{dN_1}{dt} = 0\\) or \\(\\frac{dN_2}{dt} = 0\\))  If the isoclines cross, that is where coexistence is possible!Lotka, have beautiful isoclines\nWe can find the isoclines analytically! Basically, how the population of each species change is based on their own density as well as the other. So to analyze that visually, we should put both species on the axes: \\(N_1\\) on the x-axis and \\(N_2\\) on the y-axis. A visual representation of what I mean:\n\nslate &lt;- ggplot(data = NULL) + \n  xlab(expression(N[1])) + \n  ylab(expression(N[2])) + \n  theme_classic() +\n  theme(axis.text = element_text(size = 14, color=\"black\"),\n        axis.title = element_text(size = 15, color = \"black\"));\nslate\n\n\n\n\n\n\n\n\nTo find the isocline, set \\(\\frac{dN}{dt} = 0\\) and we get:\n\\[\n\\frac{1 -  c_{21} N_2}{c_{11}} =  N_1\n\\] By generating different abundances for \\(N_2\\), we can see what the \\(N_1\\) has to be for its isocline to be 0!\n\nisocline_N1 &lt;- function(c_df) {\n  ab &lt;- as.numeric(seq(0, 200))\n  c_11 &lt;- as.numeric(c_df[\"c_11\"])\n  c_21 &lt;- as.numeric(c_df[\"c_21\"])\n\n  isocline &lt;- (1 - (c_21 * ab)) / c_11\n\n  full &lt;- cbind.data.frame(N2 = ab, N1 = isocline)\n  return(full)\n}\n\nisoN1 &lt;- isocline_N1(c_df)\n\n\nslate + geom_line(data =isoN1,aes(x= N1, y= N2), color = \"#F36B23\") \n\n\n\n\n\n\n\n\nSame idea for the \\(N_2\\):\n\\[\nN_2  = \\frac{1- c_{12}N_1}{c_{22} }\n\\]\n\nisocline_N2 &lt;- function(c_df){\n  ab &lt;- as.numeric(seq(0,200))\n  c_22 &lt;- as.numeric(c_df[\"c_22\"])\n  c_12 &lt;- as.numeric(c_df[\"c_12\"])\n\n  isocline &lt;- (1 - (c_12 * ab))/c_22\n  \n  full &lt;- cbind.data.frame(N1 = ab,N2= isocline)\n  return(full)\n}\n\n\nisoN2 &lt;- isocline_N2(c_df)\nisocline_GG &lt;-  slate + \n  geom_line(data =isoN1,aes(x= N1, y= N2), color = \"#F36B23\") +\n  geom_line(data = isoN2, aes(x = N1, y= N2), color =\"#68B8BE\") ;\nisocline_GG\n\n\n\n\n\n\n\n\nWhere the isoclines cross is when \\(\\frac{dN_1}{dt}\\) and \\(\\frac{dN_2}{dt}\\) are 0 - that’s coexistence! Let’s check if our calculation of the coexistence equilibirum point is right? If it’s right, our point should be directly on where the isoclines cross.\n\ncalculate_coexistence &lt;- function(c_df){\n  c_11 &lt;- as.numeric(c_df[\"c_11\"])\n  c_12 &lt;- as.numeric(c_df[\"c_12\"])\n  c_22 &lt;- as.numeric(c_df[\"c_22\"])\n  c_21 &lt;- as.numeric(c_df[\"c_21\"])\n  \n  denom &lt;- (c_22 * c_11 - c_12 * c_21)\n  \n  N1_eq&lt;- (c_22 - c_21)/denom\n  N2_eq&lt;- (c_11 - c_12)/denom\n\n\n  return(cbind.data.frame(N1 = N1_eq, N2 = N2_eq))\n  }\n\n\ncoexist_df&lt;- calculate_coexistence (c_df)\n\n\nisocline_GG + geom_point(data = coexist_df, aes(x = N1, y= N2), size = 4)\n\n\n\n\n\n\n\n\nCool! But seems awfully empty."
  }
]